{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain==0.1.13 openai==1.14.2 ragas==0.1.7 langchain-openai==0.1.1 langchain-cohere==0.1.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_cohere.embeddings import CohereEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-3.5-turbo-0125\"\n",
    "    )\n",
    "\n",
    "embed_model=CohereEmbeddings(\n",
    "    cohere_api_key = CO_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've got an [example dataset](https://huggingface.co/datasets/explodinggradients/fiqa/viewer/ragas_eval?row=1) we'll use in the next several videos in my Hugging Face repo. \n",
    "\n",
    "You don't need to sign-up for a Hugging Face account to download the repo, but if you do end up creating an acocunt [feel free to follow me](https://huggingface.co/harpreetsahota)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"explodinggradients/fiqa\", split='baseline', trust_remote_code=True)\n",
    "\n",
    "dataset = dataset.rename_column(\"ground_truths\", \"ground_truth\")\n",
    "\n",
    "# Function to concatenate list of strings into a single string\n",
    "def flatten_list_of_strings(example):\n",
    "    # Adjust 'your_list_column' to the actual column name holding the list of strings\n",
    "    example['ground_truth'] = ' '.join(example['ground_truth'])\n",
    "    return example\n",
    "\n",
    "# Apply the function to each example in the dataset\n",
    "dataset = dataset.map(flatten_list_of_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ragas` expects the ground truth to be a string to calculaute this metric, for whatever reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {\"ground_truth\": \" \".join(x[\"ground_truth\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Context Recall\n",
    "\n",
    "[Context recall](https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_context_recall.py) is a metric for evaluating information retrieval systems. \n",
    "\n",
    "It assesses the effectiveness of a retriever in gathering all relevant context for accurately answering a question. By comparing each part of a provided ground truth answer to the retrieved information, context recall assesses the completeness of the retrieval process. \n",
    "\n",
    "- üìè **Key Point**: The goal is to have a context that backs every part of the answer.\n",
    "\n",
    "- üõ† **Necessity of Ground Truth**: Essential for measuring how well the context aligns with the correct answer.\n",
    "\n",
    "- üîç **Evaluation Process**: Each sentence in the ground truth is compared with the retrieved context to find matches.\n",
    "\n",
    "- üìà **Scoring Scale**: Ranges from 0 to 1, where values closer to 1 indicate better performance. A perfect score means the system fetched all the needed context to support the ground truth answer comprehensively.\n",
    "\n",
    "- üîÑ **Operational Mechanism**: Breaks down the answer into sentences to check for their presence in the context, aiming for a complete match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does this work?\n",
    "\n",
    "Context Recall assess how well the sentences in an answer can be attributed to the given context, focusing on identifying the sentences that are directly supported by the context (True Positives) and those that are not found in the context (False Negatives). \n",
    "\n",
    "This metric evaluates the completeness of an answer in relation to the provided context, essentially measuring the recall of the context within the answer.\n",
    "\n",
    " - For each question-context-answer set, the `CONTEXT_RECALL_RA` prompt instructs the LLM to analyze each sentence in the answer and classify whether it can be directly attributed to the given context. \n",
    "\n",
    " - The classification is binary: \"Yes\" (1) for sentences that can be attributed to the context and \"No\" (0) for those that cannot. Each classification must also include a reason.\n",
    "\n",
    " - The LLM evaluates the answer in the context of the provided information and classifies each sentence based on its presence or absence in the given context. \n",
    " \n",
    "  - This step produces classifications of sentences as either attributed to the context or not, along with reasons for each classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import context_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_recall.context_recall_prompt.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Context Recall\n",
    "\n",
    "The score is computed by taking the ratio of sentences attributed to the context (True Positives) to the total number of sentences analyzed. \n",
    "\n",
    "This reflects the proportion of the answer that can be directly traced back to the provided context, effectively measuring the recall of the context in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    llm=llm,\n",
    "    embeddings=embed_model,\n",
    "    metrics=[context_recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "\n",
    "**Input:** A dataset containing trios of questions, contexts, and ground truth answers.\n",
    "\n",
    "**Process:**\n",
    "\n",
    "- **Step 1**: For each question-context-answer set, the LLM looks at each sentence in the answer and classify whether it can be directly attributed to the given context. \n",
    "\n",
    "- **Step 2**: The LLM classifies sentences as either attributed to the context or not. The classification is binary: \"Yes\" (1) for sentences that can be attributed to the context and \"No\" (0) for those that cannot. \n",
    "\n",
    "- **Step 3**: The score is computed by taking the ratio of sentences attributed to the context (True Positives) to the total number of sentences considered. \n",
    "\n",
    "**Output**: A score representing the context recall of the answer, quantifying the extent to which the information provided in the answer is supported by the given context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
