{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Context Recall\n",
    "The key is having context that supports all parts of the answer.\n",
    "What It Measures: Context recall is all about making sure the retrieved context is in sync with the ground truth answer. You're aiming for a perfect match here.\n",
    "\n",
    "Scoring: It's scored on a scale of 0 to 1. The closer you get to 1, the better your model is at aligning the context with the answer.\n",
    "\n",
    "How It Works: The model breaks down the ground truth answer sentence by sentence. For each sentence, it checks if it can be traced back to the retrieved context. Ideally, every sentence should have a corresponding piece of context.\n",
    "\n",
    "Example:\n",
    "\n",
    "‚ùì Question: Where is France and what is its capital?\n",
    "\n",
    "üü¢ Ground Truth: France is in Western Europe and its capital is Paris.\n",
    "\n",
    "‚¨ÜÔ∏è High Recall: France, in Western Europe. Paris, its capital.\n",
    "\n",
    "‚¨áÔ∏è Low Recall: France, in Western Europe, known for wines and cuisine."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
