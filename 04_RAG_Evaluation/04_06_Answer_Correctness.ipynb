{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain==0.1.13 openai==1.14.2 ragas==0.1.7 langchain-openai==0.1.1 langchain-cohere==0.1.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_cohere.embeddings import CohereEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-3.5-turbo-0125\"\n",
    "    )\n",
    "\n",
    "embed_model=CohereEmbeddings(\n",
    "    cohere_api_key = CO_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've got an [example dataset](https://huggingface.co/datasets/explodinggradients/fiqa/viewer/ragas_eval?row=1) we'll use in the next several videos in my Hugging Face repo. \n",
    "\n",
    "You don't need to sign-up for a Hugging Face account to download the repo, but if you do end up creating an acocunt [feel free to follow me](https://huggingface.co/harpreetsahota)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"explodinggradients/fiqa\", split='baseline', trust_remote_code=True)\n",
    "\n",
    "dataset = dataset.rename_column(\"ground_truths\", \"ground_truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ragas` expects the ground truth to be a string to calculaute this metric, for whatever reason. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {\"ground_truth\": \" \".join(x[\"ground_truth\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ **Answer Correctness**\n",
    "\n",
    "[Answer correctness](https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_answer_correctness.py) measures how closely a model's response aligns with the established ground truth, focusing on precision and accuracy.\n",
    "\n",
    "- üìè **Scoring Range**: Scores range from 0 to 1, where scores near 1 indicate a high level of alignment with the correct answer.\n",
    "\n",
    "- üèóÔ∏è **Weighted Approach**: Integrates semantic and factual similarities for a comprehensive correctness score.\n",
    "\n",
    "- üîÑ **Threshold Application**: Allows evaluators to apply a threshold, turning the nuanced score into a binary outcome for easier interpretation.\n",
    "\n",
    "- üîç **Focus on Accuracy**: Ensures that generated responses are not only relevant but factually and semantically correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does this work?\n",
    "\n",
    "Answer Correctness evaluates an answer's correctness by assessing its factual alignment with a ground truth and its overall semantic coherence. \n",
    "\n",
    "- The system starts by using the `CORRECTNESS_PROMPT`, which instructs the LLM to analyze each statement in the answer for its factual alignment with the ground truth, then classify each statements in the given answer into three categories:\n",
    "\n",
    "    - **‚úÖ TP (True Positive):** Facts or statements that are present in both the ground truth and the generated answer.\n",
    "\n",
    "    - **‚ùå FP (False Positive):** Facts or statements that are present in the generated answer but not in the ground truth.\n",
    "\n",
    "    - **‚ùé FN (False Negative):** Facts or statements that are present in the ground truth but not in the generated answer.\n",
    "\n",
    "\n",
    "You can, optionally, pass a list of two floats that sum to one to the `weights` argument.\n",
    "\n",
    "- ‚öñÔ∏è **Factual Accuracy Weight**: Assigned to evaluate the classification of statements as True Positives, False Positives, and False Negatives.\n",
    "\n",
    "- üß† **Semantic Similarity Weight**: Focuses on assessing how closely the answer captures the essence and nuances of the ground truth.\n",
    "\n",
    "- üîç **Semantic Evaluation**: If semantic similarity has a non-zero weight, the system evaluates the overall meaning of the answer compared to the ground truth, capturing nuances beyond factual accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import answer_correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_correctness.correctness_prompt.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Context Precision Score\n",
    "\n",
    "The system computes an F1 score based on the TP, FP, and FN classified by the LLM. \n",
    "\n",
    "$$F1 = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "where Precision is $$\\text{Precision} = \\frac{TP}{TP + FP}$$ and Recall is $$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "It can also be represented as:\n",
    "\n",
    "$$F1 = \\frac{TP}{TP + 0.5 \\times (FP + FN)}$$\n",
    "\n",
    "- This measures the factuality of the answer by considering the accuracy and completeness of the response relative to the ground truth.\n",
    "\n",
    "- üéöÔ∏è **Final Score Composition**: Combines the F1 score and semantic similarity score into a weighted average, based on preset weights.\n",
    "\n",
    "- üèÜ **Composite Score Aim**: Reflects the comprehensive factual and semantic alignment of the answer with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    llm=llm,\n",
    "    embeddings=embed_model,\n",
    "    metrics=[answer_correctness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input:** A dataset containing trios of questions, answers, and their corresponding ground truths.\n",
    "\n",
    "**Process:**\n",
    "\n",
    "  - **Step 1:** Use the correctness prompt to guide the LLM in classifying statements from the answer into TP, FP, and FN based on their factual alignment with the ground truth.\n",
    "\n",
    "  - **Step 2 (Conditional):** Evaluate the semantic similarity between the answer and the ground truth if semantic analysis is weighted.\n",
    "\n",
    "  - **Step 3:** Calculate the F1 score from the TP, FP, and FN classifications to assess factual accuracy.\n",
    "\n",
    "  - **Step 4:** Compute the final score by averaging the F1 and semantic similarity scores according to their weights, providing a comprehensive measure of answer correctness.\n",
    "  \n",
    "**Output:** A composite score that quantifies the correctness of an answer, reflecting both its factual accuracy and semantic alignment with the ground truth. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
