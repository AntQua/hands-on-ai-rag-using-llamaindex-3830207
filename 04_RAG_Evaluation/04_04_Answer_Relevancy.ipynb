{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain==0.1.13 openai==1.14.2 ragas==0.1.6 langchain-openai==0.1.1 langchain-cohere==0.1.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_cohere.embeddings import CohereEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-3.5-turbo-0125\"\n",
    "    )\n",
    "\n",
    "embed_model=CohereEmbeddings(\n",
    "    cohere_api_key = CO_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've got an [example dataset](https://huggingface.co/datasets/explodinggradients/fiqa/viewer/ragas_eval?row=1) we'll use in the next several videos in my Hugging Face repo. \n",
    "\n",
    "You don't need to sign-up for a Hugging Face account to download the repo, but if you do end up creating an acocunt [feel free to follow me](https://huggingface.co/harpreetsahota)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"explodinggradients/fiqa\", split='baseline', trust_remote_code=True)\n",
    "\n",
    "dataset.rename_column(\"ground_truths\", \"ground_truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç **Answer Relevancy**\n",
    "\n",
    "- üéØ Answer Relevancy measures how directly an answer addresses the question asked.\n",
    "\n",
    "- üîç The process involves generating hypothetical questions from the answer and comparing these to the original question to assess similarity.\n",
    "\n",
    "- üìç Its core focus is on identifying answers that precisely address the query without veering off-topic.\n",
    "\n",
    "- üìà Scoring ranges from 0 to 1, with higher scores indicating a closer match between the answer and the question.\n",
    "\n",
    "- üèÜ The metric rewards answers that are directly applicable and penalizes those that include irrelevant details.\n",
    "\n",
    "- üìê It calculates mean cosine similarity between the original and reverse-engineered questions to quantify relevancy.\n",
    "\n",
    "$$\\text{answer relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} cos(E_{g_i}, E_o)$$\n",
    "\n",
    "\n",
    "$E_{g_i}$ is the embedding of the generated question .\n",
    "\n",
    "$E_o$ is the embedding of the original question.\n",
    "\n",
    "$N$ is the number of generated questions, which is 3 default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does this work?\n",
    "\n",
    "For each provided answer, the system asks the LLM to do two things: \n",
    "\n",
    "1. Generate a question that fits the answer. \n",
    "\n",
    "2. Classify if the answer is \"noncommittal\" (evasive or not directly answering the question). \n",
    "\n",
    "The noncommittal classification is a simple 0 or 1 flag indicating if the answer directly addresses the question or dodges it. Noncommittal classification affects the relevancy score, reducing it if the answer is evasive, even if the generated question closely matches the original.\n",
    "\n",
    "This task uses the `QUESTION_GEN` prompt, effectively turning the answer (and its context) back into a question as if trying to reverse-engineer what the original question could have been. \n",
    "\n",
    "### **Question Generation and Answer Classification**\n",
    "\n",
    "  - This is a single step that accomplishes two tasks: \n",
    "    - creating a question that matches the provided answer\n",
    "    \n",
    "    - assessing the answer's directness or relevancy.\n",
    "\n",
    "   - You can specify the number of questions to generate per answer through the `strictness` argument. This attribute determines how many questions the LLM should generate for each answer, allowing for a more thorough evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import answer_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevancy.question_generation.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Computing Answer Relevancy**\n",
    "\n",
    "- üìè Similarity between the original and generated questions is measured using embeddings, specifically through cosine similarity.\n",
    "\n",
    "- üéØ The relevancy of an answer is determined by how close the embeddings of the original question are to those of the generated question(s).\n",
    "\n",
    "- üìä The final score averages the similarity scores across all generated questions, adjusted by the strictness setting.\n",
    "\n",
    "- üîª Noncommittal answers lead to a score penalty, ensuring only direct, relevant answers achieve high scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    llm=llm,\n",
    "    embeddings=embed_model,\n",
    "    metrics=[answer_relevancy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "**Input:** An original question and an answer (with context).\n",
    "\n",
    "**Process:** Use LLM to generate question(s) from the answer, classify committal status, calculate similarity between original and generated questions, and adjust based on committal status.\n",
    "\n",
    "**Output:** A relevancy score ranging from 0 to 1, with 1 indicating high relevancy (meaning the answer directly and accurately addresses the original question) and 0 indicating low relevancy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
