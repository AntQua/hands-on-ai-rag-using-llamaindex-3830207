{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain==0.1.13 openai==1.14.2 ragas==0.1.7 langchain-openai==0.1.1 langchain-cohere==0.1.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§î Imports from `ü¶úüîóLangChain`? I thought this was a `üóÇÔ∏èLlamaIndexü¶ô` Course...\n",
    "\n",
    "Yes, it most certainly is. \n",
    "\n",
    "But the state of LLMOps/LLMEval tooling is in it's infancy. Things are rapidly changing, libraries are constantly breaking...it's seriously a mess right now. So, I've only bothered to learn one library for RAG evaluation, and that's `ragas`.\n",
    "\n",
    "`ragas` itself is hacky and at times quick finnicky, but it's what we got to work with. It's tightly integrated with `LangChain`, and at the time of this writing their `LlamaIndex` is broken.\n",
    "\n",
    "So, we'll use `ragas` with the `LangChain` backend. All you're really going to be doing with `LangChain` is using an LLM and Embedding model instantiation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_cohere.embeddings import CohereEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-3.5-turbo-0125\"\n",
    "    )\n",
    "\n",
    "embed_model=CohereEmbeddings(\n",
    "    cohere_api_key = CO_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've got an [example dataset](https://huggingface.co/datasets/explodinggradients/fiqa/viewer/ragas_eval?row=1) we'll use in the next several videos in my Hugging Face repo. \n",
    "\n",
    "You don't need to sign-up for a Hugging Face account to download the repo, but if you do end up creating an acocunt [feel free to follow me](https://huggingface.co/harpreetsahota)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"explodinggradients/fiqa\", split='baseline', trust_remote_code=True)\n",
    "\n",
    "dataset = dataset.rename_column(\"ground_truths\", \"ground_truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ù **Faithfulness**\n",
    "\n",
    "- üéØ Faithfulness evaluates how accurately responses align with the provided context.\n",
    "\n",
    "- üïµÔ∏è‚Äç‚ôÇÔ∏è The process involves two steps: \n",
    "    - identifying statements within an answer\n",
    "    - verifying these statements against the context\n",
    "\n",
    "- üìä The faithfulness score, ranging from 0 to 1, measures the accuracy of statements in an answer.\n",
    "\n",
    "- üìà High faithfulness scores indicate responses that are both reliable and closely reflect the context's factual content.\n",
    "\n",
    "$$\\text{Faithfulness score} = {|\\text{Number of claims in the generated answer that can be inferred from given context}| \\over |\\text{Total number of claims in the generated answer}|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does this work?\n",
    "\n",
    "The order of operations involving calls to LLMs with specific prompts for evaluating the faithfulness of statements derived from a given context or answer. \n",
    "\n",
    "### **Generating Statements from Answers**:\n",
    "The order of operations involving calls to Language Models (LLMs) and the usage of prompts in this code focuses on evaluating the faithfulness of statements derived from a given context or answer. \n",
    "\n",
    "   - Initially, for a given question-answer pair, the system uses the `LONG_FORM_ANSWER_PROMPT`. This prompt instructs the LLM to generate one or more statements based on the given answer. The aim here is to distill the essence of the answer into concise statements.\n",
    "\n",
    "   - The prompt includes detailed instructions for the LLM, specifying how to transform answers into statements, and is accompanied by examples to guide the LLM's generation process.\n",
    "   \n",
    "   - Once the LLM processes this prompt, the generated output is parsed using the `_statements_output_parser`, which ensures the output aligns with the `StatementsAnswers` model structure. This process extracts the list of statements as structured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'long_form_answer',\n",
       " 'instruction': 'Create one or more statements from each sentence in the given answer.',\n",
       " 'output_format_instruction': 'The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"description\": \"the list of extracted statements\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).',\n",
       " 'examples': [{'question': 'Who was  Albert Einstein and what is he best known for?',\n",
       "   'answer': 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.',\n",
       "   'statements': ['Albert Einstein, a German-born theoretical physicist, is renowned for being one of the most influential physicists in history.',\n",
       "    'Albert Einstein was best known for his theory of relativity.',\n",
       "    \"Einstein's contributions significantly advanced the field of quantum mechanics\",\n",
       "    \"Recognized globally, Einstein's work has profoundly impacted the scientific community\",\n",
       "    \"Einstein's groundbreaking theories continue to shape our understanding of physics today.\"]},\n",
       "  {'question': 'Cadmium Chloride is slightly soluble in this chemical, it is also called what?',\n",
       "   'answer': 'alcohol',\n",
       "   'statements': ['Cadmium Chloride is slightly soluble in alcohol.']},\n",
       "  {'question': 'Were Hitler and Benito Mussolini of the same nationality?',\n",
       "   'answer': \"Sorry, I can't provide answer to that question.\",\n",
       "   'statements': []}],\n",
       " 'input_keys': ['question', 'answer'],\n",
       " 'output_key': 'statements',\n",
       " 'output_type': 'json',\n",
       " 'language': 'english'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faithfulness.long_form_answer_prompt.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluating Statement Faithfulness**\n",
    "\n",
    "   - After statements are generated, the next step is to evaluate **faithfulness** ‚Äîthat is, whether the statements are accurate and truthful representations of the original context or answer.\n",
    "\n",
    "   - For this, the system employs the `NLI_STATEMENTS_MESSAGE` prompt. \n",
    "   \n",
    "   - This prompt directs the LLM to assess each generated statement's faithfulness based on the original context (from which the question-answer pair was derived) or additional context provided separately. \n",
    "   \n",
    "   - It asks the LLM to return a verdict on each statement, indicating whether the statement can be verified against the context.\n",
    "\n",
    "   - The instruction to the LLM includes a detailed explanation of how to judge each statement's faithfulness, supported by examples for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'nli_statements',\n",
       " 'instruction': 'Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be verified based on the context or 0 if the statement can not be verified based on the context.',\n",
       " 'output_format_instruction': 'The output should be a well-formatted JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output JSON schema:\\n```\\n{\"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/StatementFaithfulnessAnswer\"}, \"definitions\": {\"StatementFaithfulnessAnswer\": {\"title\": \"StatementFaithfulnessAnswer\", \"type\": \"object\", \"properties\": {\"statement\": {\"title\": \"Statement\", \"description\": \"the original statement, word-by-word\", \"type\": \"string\"}, \"verdict\": {\"title\": \"Verdict\", \"description\": \"the verdict(0/1) of the faithfulness.\", \"type\": \"integer\"}, \"reason\": {\"title\": \"Reason\", \"description\": \"the reason of the verdict\", \"type\": \"string\"}}, \"required\": [\"statement\", \"verdict\", \"reason\"]}}}\\n```\\n\\nDo not return any preamble or explanations, return only a pure JSON string surrounded by triple backticks (```).',\n",
       " 'examples': [{'context': 'John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.',\n",
       "   'statements': ['John is majoring in Biology.',\n",
       "    'John is taking a course on Artificial Intelligence.',\n",
       "    'John is a dedicated student.',\n",
       "    'John has a part-time job.'],\n",
       "   'answer': [{'statement': 'John is majoring in Biology.',\n",
       "     'verdict': 0,\n",
       "     'reason': \"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\"},\n",
       "    {'statement': 'John is taking a course on Artificial Intelligence.',\n",
       "     'verdict': 0,\n",
       "     'reason': 'The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.'},\n",
       "    {'statement': 'John is a dedicated student.',\n",
       "     'verdict': 1,\n",
       "     'reason': 'The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.'},\n",
       "    {'statement': 'John has a part-time job.',\n",
       "     'verdict': 0,\n",
       "     'reason': 'There is no information given in the context about John having a part-time job.'}]},\n",
       "  {'context': 'Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.',\n",
       "   'statements': ['Albert Einstein was a genius.'],\n",
       "   'answer': [{'statement': 'Albert Einstein was a genius.',\n",
       "     'verdict': 0,\n",
       "     'reason': 'The context and statement are unrelated'}]}],\n",
       " 'input_keys': ['context', 'statements'],\n",
       " 'output_key': 'answer',\n",
       " 'output_type': 'json',\n",
       " 'language': 'english'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faithfulness.nli_statements_message.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Computing the Faithfulness Score**\n",
    "\n",
    "   - The `Faithfulness` class computes a score representing the overall faithfulness of the statements. This computation involves counting the number of statements judged as faithful (verdict = 1) and dividing by the total number of statements.\n",
    "   \n",
    "   - This score measures how faithfully the generated statements represent the given answer or context.\n",
    "\n",
    "\n",
    "\n",
    "Take a look at [the source code](https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_faithfulness.py) for the metric for full details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    llm=llm,\n",
    "    embeddings=embed_model,\n",
    "    metrics=[faithfulness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "**Input**: An answer to a question and the context related to that answer.\n",
    "\n",
    "**Process:**\n",
    "\n",
    "  - **Step 1:** Use LLM to transform the answer into a set of discrete statements.\n",
    "\n",
    "  - **Step 2:** Evaluate each statement's faithfulness using another LLM prompt, based on the original answer's context.\n",
    "\n",
    "  - **Step 3:** Calculate a score representing the percentage of statements deemed faithful.\n",
    "\n",
    "**Output:** A faithfulness score, where a score closer to 1 indicates higher faithfulness, showing that the generated statements are accurate and truthful representations of the original context or answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
