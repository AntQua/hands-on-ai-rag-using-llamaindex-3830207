{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain==0.1.13 openai==1.14.2 ragas==0.1.6 langchain-openai==0.1.1 langchain-cohere==0.1.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_cohere.embeddings import CohereEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-3.5-turbo-0125\"\n",
    "    )\n",
    "\n",
    "embed_model=CohereEmbeddings(\n",
    "    cohere_api_key = CO_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've got an [example dataset](https://huggingface.co/datasets/explodinggradients/fiqa/viewer/ragas_eval?row=1) we'll use in the next several videos in my Hugging Face repo. \n",
    "\n",
    "You don't need to sign-up for a Hugging Face account to download the repo, but if you do end up creating an acocunt [feel free to follow me](https://huggingface.co/harpreetsahota)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(\"explodinggradients/fiqa\", split='baseline', trust_remote_code=True)\n",
    "\n",
    "dataset = dataset.rename_column(\"ground_truths\", \"ground_truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü™° **Context Precision**\n",
    "\n",
    "- üéñÔ∏è [Context Precision](https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_context_precision.py) evaluates how well a model ranks relevant information at the top.\n",
    "\n",
    "- üîç It checks if essential context related to a query is prioritized, that is, a model's ability to identify key information.\n",
    "\n",
    "- üìè The assessment compares the model's context ranking to ground-truth data for alignment with the expected relevance order.\n",
    "\n",
    "- üéØ Values range from 0 to 1, with higher values indicating better precision in presenting relevant context prominently.\n",
    "\n",
    "- üìà High scores indicate the model's success in highlighting the most important context\n",
    "\n",
    "\n",
    "$$\\text{Context Precision@K} = \\frac{\\sum_{k=1}^{K} \\left( \\text{Precision@k} \\times v_k \\right)}{\\text{Total number of relevant items in the top } K \\text{ results}}$$\n",
    "\n",
    "$$\\text{Precision@k} = {\\text{true positives@k} \\over  (\\text{true positives@k} + \\text{false positives@k})}$$\n",
    "\n",
    "Where $K$ is the total number of chunks in `contexts` and $v_k \\in \\{0, 1\\}$ is the relevance indicator at rank $k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does this work?\n",
    "\n",
    "The `ContextPrecision` metric evaluates the usefulness of provided context in arriving at a given answer, with a specific focus on whether all relevant context items are appropriately used to support the answer. \n",
    "\n",
    "  - The `CONTEXT_PRECISION` prompt is defined to guide the LLM in verifying the usefulness of context for a given question-answer pair. \n",
    "\n",
    "  - This prompt asks the LLM to provide a binary verdict (1 for useful, 0 for not useful) along with a reason for its decision.\n",
    "\n",
    "  - For each question-answer-context trio, the system generates a verification task using the defined prompt. This task directs the LLM to examine the given context and determine if it was useful in reaching the provided answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import context_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, if you want to calculate this value but you don't have the ground truth response you should use `context_utilization` instead. You can import as follows:\n",
    "\n",
    "```python \n",
    "\n",
    "from ragas.metrics import context_utilization\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_precision.context_precision_prompt.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Context Precision Score\n",
    "\n",
    "With all the verdicts collected, the system calculates the average precision score. \n",
    "\n",
    "- üìã **Prepare Verdict List**: Converts input verifications into a binary list, marking 1 for useful contexts and 0 for non-useful.\n",
    "\n",
    "- ‚ûï **Calculate Numerator**: Sums up precision at each rank with a relevant context, calculated by the ratio of cumulative relevant items to the rank, multiplied by the relevance indicator.\n",
    "\n",
    "- ‚ûó **Calculate Denominator**: Total count of relevant contexts in the list, with a tiny value added to prevent division by zero.\n",
    "\n",
    "- üìä **Compute Score**: The average precision score is calculated, giving the ordered precision of context usefulness.\n",
    "\n",
    "This calculation places more importance on relevant contexts appearing early in the list, with the average precision score indicating the effectiveness of contexts in supporting answers.  A higher score indicates that the provided context was generally useful and relevant in deriving the given answers, reflecting effective context utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    llm=llm,\n",
    "    embeddings=embed_model,\n",
    "    metrics=[context_precision])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "**Input:** A dataset comprising question-answer-context trios.\n",
    "\n",
    "**Process**:\n",
    "\n",
    "  - **Step 1**: Formulate verification tasks for each trio using the `CONTEXT_PRECISION` prompt.\n",
    "\n",
    "  - **Step 2**: Evaluate each task's context usefulness with the LLM, obtaining verdicts and reasons.\n",
    "\n",
    "  - **Step 3**: Parse LLM responses into structured data.\n",
    "  \n",
    "  - **Step 4**: Calculate the average precision score based on the usefulness verdicts, emphasizing the relevance and ranking of useful context items.\n",
    "\n",
    "**Output:** An average precision score, quantifying the overall effectiveness of context utilization in providing answers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
