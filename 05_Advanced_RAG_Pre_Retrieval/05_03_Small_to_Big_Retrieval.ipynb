{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.25 llama-index-embeddings-fastembed qdrant-client llama-index-vector-stores-qdrant llama-index-llms-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(\n",
    "    api_key=CO_API_KEY, \n",
    "    model=\"command-r-plus\", \n",
    "    temperature=0.75, \n",
    "    system_prompt=\"\"\"Use ONLY the provided context and generate a complete, coherent answer to the user's query. \n",
    "    Your response must be grounded in the provided context and relevant to the essence of the user's query.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "setup_embed_model(provider=\"fastembed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_documents_from_docstore\n",
    "\n",
    "senpai_documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eval_dataset = load_dataset(\"harpreetsahota/LI_Learning_RAG_Eval_Set\", split='train')\n",
    "\n",
    "eval_dataset = eval_dataset.filter(lambda x: x['question_groundedness_score'] is not None and x['question_groundedness_score'] >= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(senpai_documents[42].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîπ‚Üíüî∑ Small to Big Retrieval\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JDA3xJul91tItEhB.png>\n",
    "\n",
    "\n",
    "The concept of small to big retrieval, also known as recursive retrieval, is a key part of LlamaIndex. \n",
    "\n",
    "- üóÇÔ∏è **Querying Stage**\n",
    "\n",
    "  - **Retriever Role**: Defines how to efficiently retrieve relevant context from an index based on a query.\n",
    "  \n",
    "  - **Relevancy and Efficiency**: The retrieval strategy is crucial for ensuring the relevance of the data and the efficiency of the retrieval process.\n",
    "  \n",
    "### Components Involved in the Retrieval Process\n",
    "\n",
    "- üîÑ **Recursive Retrieval Process**\n",
    "\n",
    "  - **Small Chunks (Child Chunks)**: Initially retrieves smaller, query-specific chunks of data.\n",
    "\n",
    "  - **Big Chunks (Parent Chunks)**: Follows references to larger, contextual chunks related to the smaller chunks.\n",
    "\n",
    "- üõ†Ô∏è **Node Postprocessor**\n",
    "\n",
    "  - **Transformation and Filtering**: Applies transformations, filtering, or re-ranking to the retrieved nodes to enhance data quality and relevance.\n",
    "  \n",
    "- üìù **Response Synthesizer**\n",
    "\n",
    "  - **Response Generation**: Uses the retrieved text chunks along with the user query to generate a response from a Language Learning Model (LLM).\n",
    "\n",
    "\n",
    "\n",
    "## ü™ü`SentenceWindowNodeParser`\n",
    "\n",
    "The `SentenceWindowNodeParser` is unique in that it focuses on individual sentences while also capturing the surrounding context.  This is particularly useful for tasks where understanding the broader context of a sentence is useful.\n",
    "\n",
    "### How it Works\n",
    "\n",
    "1. **Sentence Splitting:** \n",
    "\n",
    "    *   Similar to `SentenceSplitter`, it first divides the document into individual sentences using a sentence tokenizer (defaults to [`PunktSentenceTokenizer`](https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html) from the `nltk` library).\n",
    "\n",
    "2. **Window Creation:**\n",
    "    *   For each sentence (node), it gathers a \"window\" of surrounding sentences based on the specified `window_size`. \n",
    "\n",
    "    *   This window is stored in the node's metadata under the `window_metadata_key`.\n",
    "\n",
    "3. **Metadata Management:**\n",
    "\n",
    "    *   The original sentence text is also stored in the metadata under `original_text_metadata_key`.\n",
    "\n",
    "    *   Importantly, both the window and original text are excluded from being seen by the embedding model and LLM.\n",
    "\n",
    "### Arguments you need to know\n",
    "\n",
    "*   **`window_size`**: Controls the number of sentences to include before and after the central sentence in the window.\n",
    "\n",
    "*   **`window_metadata_key`**: The key used to store the window text in the node's metadata.\n",
    "\n",
    "*   **`original_text_metadata_key`**: The key used to store the original sentence text in the metadata.\n",
    "\n",
    "*   **`sentence_splitter`**: The text splitter to use when splitting documents (defaults to [`PunktSentenceTokenizer`](https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html) from the `nltk` library).\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "```python\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "parser = SentenceWindowNodeParser(window_size=2)\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "```\n",
    "\n",
    "### When to Use SentenceWindowNodeParser\n",
    "\n",
    "*   **Tasks requiring sentence-level understanding with context:** \n",
    "    *   Question answering, summarization, or sentiment analysis where the surrounding sentences provide valuable context.\n",
    "\n",
    "*   **Fine-grained control over embedding scope:** \n",
    "    *   Creating embeddings that focus on the specific meaning of a sentence within its local context.\n",
    "    \n",
    "*   **Combining with MetadataReplacementNodePostProcessor:**\n",
    "    *   Replacing the original sentence with its surrounding window before sending it to the LLM, allowing the model to consider the broader context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senpai_documents[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "SentenceWindowNodeParser(window_size=2).build_window_nodes_from_documents([senpai_documents[42]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentenceWindowNodeParser(window_size=3).get_nodes_from_documents([senpai_documents[42]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ **Understanding the `MetadataReplacementPostProcessor` and `SentenceWindowNodeParser`**\n",
    "\n",
    "- üìù **`SentenceWindowNodeParser` Review**\n",
    "\n",
    "  - **Single Sentence Parsing**: Parses documents into nodes, each containing a single sentence.\n",
    "\n",
    "  - **Contextual Window**: Each node includes a \"window\" of sentences surrounding the core sentence for added context.\n",
    "\n",
    "- üîÑ **`MetadataReplacementPostProcessor`**\n",
    "\n",
    "  - **Context Enhancement**: Replaces the sentence in each node with its surrounding window of sentences during retrieval.\n",
    "\n",
    "  - **Used in Conjunction**: Often paired with the `SentenceWindowNodeParser` to maximize contextual data provided to the LLM (Language Learning Model).\n",
    "\n",
    "### Query and Response Process\n",
    "\n",
    "- üîç **Query Handling**\n",
    "\n",
    "  - **Sentence Retrieval**: Retrieves the most relevant sentences based on the query.\n",
    "\n",
    "  - **Context Injection**: Instead of merely returning these sentences, the post-processor injects the surrounding context from the window.\n",
    "\n",
    "- üìä **Benefits of Enhanced Context**\n",
    "\n",
    "  - **Improved Understanding**: More context helps the LLM understand queries better, leading to more accurate responses.\n",
    "\n",
    "  - **Detailed Responses**: The additional context allows for responses that are both detailed and relevant.\n",
    "\n",
    "- üåü **Ideal for Large Documents**\n",
    "\n",
    "  - **Fine-Grained Retrieval**: Especially useful for large documents or indexes, enabling more precise information extraction.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:2000/0*JKZ9m_c6jyIKqCWu.png\">\n",
    "\n",
    "Image Source: [Ivan Ilin](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "def sentence_window_splitter(window_size, documents):\n",
    "    splitter = SentenceWindowNodeParser(\n",
    "        window_size=window_size,\n",
    "        window_metadata_key=\"window_size\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "        )\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = sentence_window_splitter(window_size=5, documents=senpai_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0].get_content(metadata_mode=\"llm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë∑üèΩ‚Äç‚ôÇÔ∏è üóÇÔ∏è Build the Index and Ingest to Qdrant\n",
    "\n",
    "We build both the sentence index, as well as the \"base\" index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from utils import create_index, create_query_engine\n",
    "from utils import setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"words-of-the-senpai-small-to-big-sentence-window\"\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = create_index(vector_store=vector_store, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ingest\n",
    "\n",
    "transforms = [Settings.embed_model]\n",
    "\n",
    "split_nodes = ingest(\n",
    "    documents=nodes,\n",
    "    transformations=transforms,\n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "from utils import create_query_engine\n",
    "from prompts import ANSWER_GEN_PROMPT\n",
    "\n",
    "ANSWER_GEN_PROMPT_TEMPLATE = PromptTemplate(ANSWER_GEN_PROMPT)\n",
    "\n",
    "node_postprocessors = [MetadataReplacementPostProcessor(target_metadata_key=\"window\")]\n",
    "\n",
    "query_engine = create_query_engine(\n",
    "    index=index, \n",
    "    mode=\"query\",\n",
    "    response_mode=\"compact\",\n",
    "    similiarty_top_k=5,\n",
    "    vector_store_query_mode=\"mmr\", \n",
    "    vector_store_kwargs={\"mmr_threshold\": 0.42},\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )\n",
    "\n",
    "query_engine.update_prompts({'response_synthesizer:text_qa_template':ANSWER_GEN_PROMPT_TEMPLATE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_pipeline\n",
    "\n",
    "chain = [Settings.llm,  query_engine]\n",
    "\n",
    "query_pipeline = create_query_pipeline(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_generations_on_eval_set\n",
    "\n",
    "smol_eval_set = eval_dataset.shuffle(seed=42).select(range(10))\n",
    "\n",
    "smol_eval_set = run_generations_on_eval_set(smol_eval_set, col_name=\"small-to-big-sentence-window-answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in smol_eval_set:\n",
    "    print(\"üí¨\\n\")\n",
    "    print(f\"\"\"üôãüèΩ‚Äç‚ôÇÔ∏è Question: {row[\"question\"]}\"\"\")\n",
    "    print(f\"\"\"\"ü§ñ Reponse: {row[\"small-to-big-sentence-window-answer\"]}\"\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë®‚Äçüë¶ Smaller Child Chunks Referring to Bigger Parent Chunk\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:2000/0*x4rMd50GP99OSDuo.png\">\n",
    "\n",
    "Source: [Ivan Ilin](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)\n",
    "\n",
    "üîó **Chunk References Explained:**\n",
    "\n",
    "- üß© **Concept**: Chunk References involve smaller chunks of data pointing to larger parent chunks, forming a hierarchical graph structure.\n",
    "  \n",
    "- üåê **Purpose**: This method is utilized in recursive retrieval to efficiently manage and access data in a structured manner.\n",
    "\n",
    "### Process During Query\n",
    "\n",
    "- üîç **During Query-Time**:\n",
    "\n",
    "  - **Small Chunk Retrieval**: Initially, smaller chunks relevant to the query are retrieved.\n",
    "\n",
    "  - **Following References**: The system then follows references to retrieve the larger parent chunks associated with these smaller chunks.\n",
    "\n",
    "- üìà **Benefits of Contextual Retrieval**:\n",
    "\n",
    "  - **Enhanced Context**: Retrieving larger chunks along with the smaller ones provides additional context.\n",
    "  \n",
    "  - **Improved Responses**: This deeper context allows for more accurate and comprehensive responses to queries.\n",
    "\n",
    "This structured approach ensures that data retrieval is both efficient and context-rich, enhancing the overall synthesis and response accuracy.\n",
    "\n",
    " The code below is creating a system where smaller chunks of text refer to the larger chunks they were created from. This allows for more context to be provided when retrieving chunks of text based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SentenceSplitter class from the llama_index.core.node_parser module\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import IndexNode\n",
    "\n",
    "# Define the sizes of chunks for sentence splitting\n",
    "sub_chunk_sizes = [128, 256, 512]\n",
    "\n",
    "# Create a list of SentenceSplitter instances with different chunk sizes\n",
    "sub_node_parsers = [SentenceSplitter(chunk_size=c, chunk_overlap=16) for c in sub_chunk_sizes]\n",
    "\n",
    "# Initialize an empty list to store all index nodes\n",
    "all_nodes = []\n",
    "\n",
    "# Iterate over each base node in the provided base_nodes list\n",
    "for base_node in base_nodes:\n",
    "    # Process each base node with every SentenceSplitter in the list\n",
    "    for n in sub_node_parsers:\n",
    "        # Get sub-nodes by splitting the base node document into smaller parts\n",
    "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
    "        # Convert each sub-node into an IndexNode and link it to the base node's ID\n",
    "        sub_inodes = [\n",
    "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
    "        ]\n",
    "        # Add the newly created index nodes to the all_nodes list\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    # Also add the original base node to the list of all nodes as an IndexNode\n",
    "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "    all_nodes.append(original_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes_dict = {n.node_id: n for n in all_nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_': '2fad5d84-612e-4c76-96dd-1e3f92428a1c',\n",
       " 'embedding': None,\n",
       " 'metadata': {'page_number': 0,\n",
       "  'file_name': '../data/almanack_of_naval_ravikant.pdf',\n",
       "  'title': 'The Almanack of Naval Ravikant',\n",
       "  'author': 'Naval Ravikant'},\n",
       " 'excluded_embed_metadata_keys': [],\n",
       " 'excluded_llm_metadata_keys': [],\n",
       " 'relationships': {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='288e9089-174b-4a84-9611-90988233564f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_number': 0, 'file_name': '../data/almanack_of_naval_ravikant.pdf', 'title': 'The Almanack of Naval Ravikant', 'author': 'Naval Ravikant'}, hash='b00d124eb1a97f077d33a4231c4fe920705ea86d2e44ac22c4447d7d6944668a'),\n",
       "  <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='de0a23e7-0882-465a-9b72-77eda6d5eb49', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='af5446fe224306514baf23b9a4295b77d14ca8f4365b30d20d7686a4369a9476')},\n",
       " 'text': 'UNDERSTAND HOW WEALTH IS CREATED I like to think that if I lost all my money and you dropped me on a random street in any English-speaking country, within five or ten years Id be wealthy again because its just a skillset Ive developed that anyone can develop. Its not really about hard work. You can work in a restaurant eighty hours a week, and youre not going to get rich.',\n",
       " 'start_char_idx': 0,\n",
       " 'end_char_idx': 374,\n",
       " 'text_template': '{metadata_str}\\n\\n{content}',\n",
       " 'metadata_template': '{key}: {value}',\n",
       " 'metadata_seperator': '\\n',\n",
       " 'index_id': '288e9089-174b-4a84-9611-90988233564f',\n",
       " 'obj': None}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nodes[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ingest\n",
    "\n",
    "COLLECTION_NAME = \"words-of-the-senpai-small-to-big-parent-child\"\n",
    "\n",
    "parent_child_vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "parent_child_storage_context = StorageContext.from_defaults(vector_store=parent_child_vector_store)\n",
    "\n",
    "parent_child_index = create_index(vector_store=parent_child_vector_store, storage_context=parent_child_storage_context)\n",
    "\n",
    "transforms = [Settings.embed_model]\n",
    "\n",
    "parent_child_nodes = ingest(\n",
    "    documents=all_nodes,\n",
    "    transformations=transforms,\n",
    "    vector_store=parent_child_vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "\n",
    "parent_child_chunk = parent_child_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "retriever_chunk = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": parent_child_chunk},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "query_engine_chunk = RetrieverQueryEngine.from_args(retriever_chunk, llm=Settings.llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_child_query_engine = create_query_engine(\n",
    "    index=index, \n",
    "    mode=\"query\",\n",
    "    response_mode=\"compact\",\n",
    "    similiarty_top_k=5,\n",
    "    vector_store_query_mode=\"mmr\", \n",
    "    vector_store_kwargs={\"mmr_threshold\": 0.42},\n",
    "    node_postprocessors=node_postprocessors\n",
    "    )\n",
    "\n",
    "query_engine.update_prompts({'response_synthesizer:text_qa_template':ANSWER_GEN_PROMPT_TEMPLATE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/run-llama/llama_index/blob/aa13d47444692faa06b5753b7451b1920837b29c/llama-index-core/llama_index/core/retrievers/recursive_retriever.py#L22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
