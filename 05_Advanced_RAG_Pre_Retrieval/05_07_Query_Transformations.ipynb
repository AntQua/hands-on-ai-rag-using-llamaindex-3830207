{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip install llama-index-retrievers-bm25 llama-index==0.10.37 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-openai==0.1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /opt/conda/envs/lil_llama_index/lib/python3.10/site-\n",
      "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "\u001b[32m2025-02-05 18:43:40.247\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mfastembed.embedding\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[33m\u001b[1mDefaultEmbedding, FlagEmbedding, JinaEmbedding are deprecated.Use from fastembed import TextEmbedding instead.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = \":memory:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"openai\", \n",
    "    model=\"gpt-4o\", \n",
    "    api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "setup_embed_model(\n",
    "    provider=\"openai\", \n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=OPENAI_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from utils import get_documents_from_docstore, group_documents_by_author, sample_documents\n",
    "\n",
    "documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "documents_by_author = group_documents_by_author(documents)\n",
    "\n",
    "senpai_documents = sample_documents(documents_by_author, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Qdrant Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from utils import create_index, create_query_engine, ingest, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"query-transforms\"\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=16)\n",
    "\n",
    "nodes = ingest(\n",
    "    documents=senpai_documents,\n",
    "    transformations=[sentence_splitter, Settings.embed_model],\n",
    "    vector_store=vector_store\n",
    ")\n",
    "index = create_index(\n",
    "    from_where=\"vector_store\",\n",
    "    vector_store=vector_store,\n",
    "    embed_model=Settings.embed_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Transformation\n",
    "\n",
    "When handling user queries in a RAG system, agent, or any other pipeline, there are various ways to transform and decompose the queries before executing them.\n",
    "\n",
    "One way is query rewriting. This involves rewriting the original query in multiple ways while which then sent sent for retrieval and generation. \n",
    "\n",
    "LlamaIndex implements various query transformations, [check the source code for details](https://github.com/run-llama/llama_index/blob/f116d75557d6867ed2cc61811a1c2f0b0c4d4ddb/llama-index-legacy/llama_index/legacy/indices/query/query_transform/base.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "QUERY_GEN_PROMPT = \"\"\"Users aren't always the best at articulating what they're looking for. Your task is to understand the \n",
    "essense of the user query and generate {num_queries} alternate queries to expand the users query so it's more robust. This way the user will\n",
    "recieve the most relevant information. \n",
    "\n",
    "Examples are delimited by triple backticks (```) below\n",
    "\n",
    "````\n",
    "User Query: How can I find the positive in situations that seem negative?\n",
    "\n",
    "Alternate Queries:\n",
    "\n",
    "1. How can I cultivate optimism and positive thinking in my daily life?\n",
    "2. Is it possible to find meaning and purpose in challenging or difficult times?\n",
    "3. What are some effective strategies for reframing negative thoughts into positive ones?\n",
    "````\n",
    "\n",
    "````\n",
    "User Query: How do I deal with setbacks, failures, delays, defeat, or other disasters?\n",
    "\n",
    "Alternate Queries:\n",
    "\n",
    "1. How can I build resilience and learn to cope with adversity effectively?\n",
    "2. What are some practical tips for overcoming challenges and obstacles that I face?\n",
    "3. How can I develop a growth mindset and view setbacks as opportunities for learning?\n",
    "4. What are healthy ways to process and learn from failures and mistakes?\n",
    "````\n",
    "````\n",
    "User Query: How can I overcome defeat and suffering by changing my mindset?\n",
    "\n",
    "Alternate Queries:\n",
    "\n",
    "1. What is the power of positive thinking and affirmations, and how can they benefit me?\n",
    "2. Can mindfulness and meditation practices improve my mental well-being and outlook?\n",
    "3. How can I develop self-compassion and acceptance, especially during difficult times?\n",
    "```\n",
    "\n",
    "Generate {num_queries} alternate queries, one on each line, for the following user query:\\n\n",
    "--------------------\n",
    "User Query: {query}\\n\n",
    "--------------------\n",
    "\n",
    "Alternate Queries:\\n\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "QUERY_GEN_PROMPT_TEMPLATE = PromptTemplate(QUERY_GEN_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. What strategies can I use to increase my chances of success and seize opportunities?\n",
      "2. How can I develop a proactive mindset to attract positive outcomes in my life?\n",
      "3. Are there habits or behaviors that can enhance my ability to create favorable circumstances?\n",
      "4. How can networking and building relationships contribute to creating my own luck?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1. What strategies can I use to increase my chances of success and seize opportunities?',\n",
       " '2. How can I develop a proactive mindset to attract positive outcomes in my life?',\n",
       " '3. Are there habits or behaviors that can enhance my ability to create favorable circumstances?',\n",
       " '4. How can networking and building relationships contribute to creating my own luck?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUERY_STRING = \"How can I create my own luck?\"\n",
    "\n",
    "def generate_queries(query= QUERY_STRING, llm=Settings.llm, num_queries  = 4):\n",
    "    response = llm.predict(\n",
    "        QUERY_GEN_PROMPT_TEMPLATE, \n",
    "        num_queries=num_queries, \n",
    "        query=query\n",
    "        )\n",
    "    queries = response.split(\"\\n\")\n",
    "    queries_str = \"\\n\".join(queries)\n",
    "    print(f\"Generated queries:\\n{queries_str}\")\n",
    "    return queries\n",
    "\n",
    "generate_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `SubQuestionQueryEngine`\n",
    "\n",
    "The `SubQuestionQueryEngine` works by breaking down a complex query into simpler sub-questions (with each potentially targeting a specific data source).\n",
    "\n",
    "#### Here's how it works:\n",
    "\n",
    " - The `SubQuestionQueryEngine` receives a complex query.\n",
    "\n",
    "- It then decomposes this query into several sub-questions. Each sub-question is designed to extract specific information from a particular data source.\n",
    "\n",
    "- The engine then sends these sub-questions to their respective data sources and gathers the responses.\n",
    "\n",
    "- Finally, it synthesizes all the intermediate responses to form a final comprehensive answer to the original complex query.\n",
    "\n",
    "This process makes the `SubQuestionQueryEngine` particularly useful for handling compare/contrast queries across documents, as well as queries pertaining to a specific document. It's also well-suited for multi-document queries and can execute any number of sub-queries against any subset of query engine tools before synthesizing the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "from prompts import HYPE_ANSWER_GEN_PROMPT\n",
    "\n",
    "HYPE_ANSWER_GEN_PROMPT_TEMPLATE = PromptTemplate(HYPE_ANSWER_GEN_PROMPT)\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=index.as_query_engine(),\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"the senpais\",\n",
    "            description=\"The collective thoughts and writings of all my virtual mentors\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "sub_question_query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    use_async=True\n",
    "    )\n",
    "\n",
    "sub_question_query_engine.update_prompts({'response_synthesizer:text_qa_template':HYPE_ANSWER_GEN_PROMPT_TEMPLATE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " **Prompt Key**: question_gen:question_gen_prompt\n",
       "**Text:**\n",
       "```\n",
       "You are a world class state of the art agent.\n",
       "\n",
       "You have access to multiple tools, each representing a different data source or API.\n",
       "Each of the tools has a name and a description, formatted as a JSON dictionary.\n",
       "The keys of the dictionary are the names of the tools and the values are the descriptions.\n",
       "Your purpose is to help answer a complex user question by generating a list of sub questions that can be answered by the tools.\n",
       "\n",
       "These are the guidelines you consider when completing your task:\n",
       "* Be as specific as possible\n",
       "* The sub questions should be relevant to the user question\n",
       "* The sub questions should be answerable by the tools provided\n",
       "* You can generate multiple sub questions for each tool\n",
       "* Tools must be specified by their name, not their description\n",
       "* You don't need to use a tool if you don't think it's relevant\n",
       "\n",
       "Output the list of sub questions by calling the SubQuestionList function.\n",
       "\n",
       "## Tools\n",
       "```json\n",
       "{tools_str}\n",
       "```\n",
       "\n",
       "## User Question\n",
       "{query_str}\n",
       "\n",
       "```\n",
       "\n",
       "**Prompt Key**: response_synthesizer:text_qa_template\n",
       "**Text:**\n",
       "```\n",
       "You're a trusted mentor to an adult mentee. Your mentee is seeking advice in the form of a question.\n",
       "\n",
       "Below is your mentee's question:\n",
       "\n",
       "----------------------\n",
       "{query_str}\n",
       "----------------------\n",
       "\n",
       "You have some raw thoughts which you must use to formulate an answer to your mentee's question. Below are your thoughts:\n",
       "\n",
       "----------------------\n",
       "{context_str}\n",
       "----------------------\n",
       "\n",
       "Reflect on the question and your raw thoughts, then answer your mentee's question. Your response must be based on your raw thoughts, not on prior knowledge. \n",
       "\n",
       "DO NOT use any qualifiers, relative clauses, or introductory modifiers in your answer. Provide your answer question using the second person\n",
       "perspective, speaking directly to your mentee, in the form of a OG mentor who has been there and done that and is now coming back with the\n",
       "facts and giving them back to you. Use a HYPE tone and be straight up with your mentee! REMEMBER: Your response must be based on your raw thoughts, not on prior knowledge.\n",
       "\n",
       "```\n",
       "\n",
       "**Prompt Key**: response_synthesizer:refine_template\n",
       "**Text:**\n",
       "```\n",
       "The original query is as follows: {query_str}\n",
       "We have provided an existing answer: {existing_answer}\n",
       "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
       "------------\n",
       "{context_msg}\n",
       "------------\n",
       "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
       "Refined Answer: \n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import display_prompt_dict\n",
    "\n",
    "sub_q_prompts = sub_question_query_engine.get_prompts()\n",
    "\n",
    "display_prompt_dict(sub_q_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[the senpais] Q: What are some strategies for building one's own luck?\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[the senpais] Q: What are the different types of luck one can pursue?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[the senpais] Q: How can one hack luck to increase positive outcomes?\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[the senpais] Q: What are effective ways to minimize exposure to downside risks while maintaining skin in the game?\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='Empty Response', source_nodes=[], metadata=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_question_query_engine.query(\"How can I build my own luck, what are the types of luck I should pursue, and how can I hack luck and minimize my exposure to downside while maintaining skin in the game?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "At a high level, [HyDE](https://arxiv.org/pdf/2212.10496.pdf) is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example. \n",
    "\n",
    "- 🧐 **Problem Tackled**: Addresses the struggle of creating fully zero-shot dense retrieval systems without relevance labels.\n",
    "\n",
    "- 📚 **Traditional Methods**: Rely on relevance labels for document retrieval based on semantic similarities.\n",
    "\n",
    "- 🚫 **Zero-Shot Challenge**: Especially tough without a large dataset for training.\n",
    "\n",
    "### What is HyDE?\n",
    "\n",
    "Given a query, `HyDE` instructs a language model to generate a hypothetical document.\n",
    "\n",
    "This document captures relevance patterns but might contain inaccuracies or false details.\n",
    "\n",
    "After generating the hypothetical document, an unsupervised contrastively learned encoder encodes the document into an embedding vector.\n",
    "\n",
    "This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity.\n",
    "\n",
    "### How Does HyDE Work?\n",
    "\n",
    "The process starts by feeding a query to a generative model with the instruction to \"write a document that answers the question\". This generates a hypothetical document that captures the essence of relevance.\n",
    "\n",
    " - Generates an embedding vector for a \"fake\" document\n",
    "\n",
    "- It does not generate any actual text content for the document\n",
    "\n",
    "- The embedding is solely for reserving space in the vectorstore index\n",
    "\n",
    "- There is no full hypothetical document text you can access later\n",
    "\n",
    "This vector is used to search against the corpus embeddings, and the most similar real documents are retrieved. The idea is that a hypothetical answer to a question is more semantically similar to the real answer than the question is. \n",
    "\n",
    "**In practice this means that your search would use GPT to generate a hypothetical answer, then embed that and use it for search**.\n",
    "\n",
    "Key advantages of HyDE:\n",
    "\n",
    "- Zero-shot, no labeled data or fine-tuning needed\n",
    "\n",
    "- Performs comparably to fine-tuned retrievers across tasks/languages\n",
    "\n",
    "- Grounds the query in real data via generated hypothetical documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "\n",
    "from llama_index.core.query_engine import TransformQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde = HyDEQueryTransform(\n",
    "    include_original=True,\n",
    "    )\n",
    "\n",
    "hyde_query_engine = TransformQueryEngine(\n",
    "    query_engine = index.as_query_engine(), \n",
    "    query_transform = hyde,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " **Prompt Key**: query_transform:hyde_prompt\n",
       "**Text:**\n",
       "```\n",
       "Please write a passage to answer the question\n",
       "Try to include as many key details as possible.\n",
       "\n",
       "\n",
       "{context_str}\n",
       "\n",
       "\n",
       "Passage:\"\"\"\n",
       "\n",
       "```\n",
       "\n",
       "**Prompt Key**: query_engine:response_synthesizer:text_qa_template\n",
       "**Text:**\n",
       "```\n",
       "Context information is below.\n",
       "---------------------\n",
       "{context_str}\n",
       "---------------------\n",
       "Given the context information and not prior knowledge, answer the query.\n",
       "Query: {query_str}\n",
       "Answer: \n",
       "```\n",
       "\n",
       "**Prompt Key**: query_engine:response_synthesizer:refine_template\n",
       "**Text:**\n",
       "```\n",
       "The original query is as follows: {query_str}\n",
       "We have provided an existing answer: {existing_answer}\n",
       "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
       "------------\n",
       "{context_msg}\n",
       "------------\n",
       "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
       "Refined Answer: \n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prompt_dict(hyde_query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Creating your own luck involves applying specific knowledge with leverage and being patient over a long timescale. It's important to position yourself with accountability and an authentic skill set, aiming to be the best at what you do. Enjoying the process and consistently putting in the time and effort are crucial. Additionally, improving decision-making skills by collecting mental models and being more rational can lead to nonlinear returns in life.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = hyde_query_engine.query(QUERY_STRING)\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import InputComponent\n",
    "from utils import create_query_pipeline\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "hyde_chain = [input_component,  hyde_query_engine]\n",
    "\n",
    "hyde_query_pipeline = create_query_pipeline(hyde_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 8e918222-3a5d-4e17-bcec-b5cfe84e0b4d with input: \n",
      "input: What should I do if I feel like I am not being true to myself and relying too much on external securities?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 64777f8f-7724-4319-ad15-f5015f9cfd86 with input: \n",
      "input: What should I do if I feel like I am not being true to myself and relying too much on external securities?\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response=\"To address the feeling of not being true to yourself and relying too much on external securities, it's important to create an atmosphere of freedom that allows you to explore and discover what is genuinely true for you. Focus on understanding your own experiences rather than conforming to external expectations. Discard what is trivial and ornamental in your life, and seek the ultimate truth that is free from symbols and styles. By doing so, you can face the world with a deeper understanding and authenticity.\", source_nodes=[NodeWithScore(node=TextNode(id_='60128bd1-616f-499e-b122-29daf4299c1c', embedding=None, metadata={'page_number': 195, 'file_name': '../data/striking-thoughts.pdf', 'title': 'Striking Thoughts', 'author': 'Bruce Lee'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1261ef4e-ce70-48ce-8663-9fa758f6fa61', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_number': 195, 'file_name': '../data/striking-thoughts.pdf', 'title': 'Striking Thoughts', 'author': 'Bruce Lee'}, hash='b884765904d3a6097016367b3625a7e423b3e388656db3d591e84c7811fe2ac2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='163f2a2b-4845-44fb-8ecc-fb5e0c31d663', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b8e8de123deceadd6868b78352aa163d405602e41b5f2798e0da2e2e21a861e8')}, text='fixed,dead,therecanbeaway,adefinitepath,butnottothatwhichisliving. Truth is outside molds and patterns. - Truth exists outside of all molds and patterns, and awareness is never exclusive. Truth is never a set idea and definitelynotaconclusion.Stylesandmethodsareconclusionsbutthetruthof lifeisaprocess. Find out for yourself what is true. - Create immediately an atmosphere of freedomsothatyoucanliveandfindoutforyourselveswhatistrue,sothatyou areabletofacetheworldwiththeabilitytounderstandit,notjustconformtoit. Onecantellforoneselfwhetherthewateriswarmorcold.Inthesameway,a manmustconvincehimselfabouttheseexperiences,onlythenaretheyreal. Discard what is ornamental. - Shun what is trivial and discard what is ornamental. Theultimatetruth.-Theultimatehasnosymbol,nostyle,nosuperhuman.', mimetype='text/plain', start_char_idx=0, end_char_idx=787, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.36109236985288856), NodeWithScore(node=TextNode(id_='94dd6e70-8700-4b19-8b1e-e65271a8e651', embedding=None, metadata={'page_number': 87, 'file_name': '../data/taoofseneca_vol3.pdf', 'title': 'Letters From a Stoic Volume 3', 'author': 'Seneca'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6043e02f-18cd-4082-9936-4cf6fa5393be', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_number': 87, 'file_name': '../data/taoofseneca_vol3.pdf', 'title': 'Letters From a Stoic Volume 3', 'author': 'Seneca'}, hash='8a7b2dbd8ecbe5f83b0772ad1179f8df852cfb6a341ef89f2477f0e362c1bc5a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c1839651-1148-4322-bad3-81e1d09d042f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_number': 87, 'file_name': '../data/taoofseneca_vol3.pdf', 'title': 'Letters From a Stoic Volume 3', 'author': 'Seneca'}, hash='2f870a7ba7715ecee847fab5772cf8588130e20a2ed04e56c4276df66a49e16b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='94e2ab86-e874-4d3c-adf5-b5440a377bd9', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='365f9246def4dc7d7a975f43e2367ffa3b8397415b0e22436c1caba6360147e1')}, text='How, then, shall we avoid this vacillation? In one way onlyif there be no reaching forward in our life, if it is withdrawn into itself. For he only is anxious about the future, to whom the present is unprofitable. But when I have paid my soul its due, when a soundly-balanced mind knows that a day differs not a whit from eternitywhatever days or problems the future may bringthen the soul looks forth from lofty heights and laughs heartily to itself when it thinks upon the ceaseless succession of the ages. For what disturbance can result from the changes and the instability of Chance, if you are sure in the face of that which is unsure? Therefore, my dear Lucilius, begin at once to live, and count each separate day as a separate life.', mimetype='text/plain', start_char_idx=853, end_char_idx=1594, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.359103130001445)], metadata={'60128bd1-616f-499e-b122-29daf4299c1c': {'page_number': 195, 'file_name': '../data/striking-thoughts.pdf', 'title': 'Striking Thoughts', 'author': 'Bruce Lee'}, '94dd6e70-8700-4b19-8b1e-e65271a8e651': {'page_number': 87, 'file_name': '../data/taoofseneca_vol3.pdf', 'title': 'Letters From a Stoic Volume 3', 'author': 'Seneca'}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_query_pipeline.run(input=\"What should I do if I feel like I am not being true to myself and relying too much on external securities?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
