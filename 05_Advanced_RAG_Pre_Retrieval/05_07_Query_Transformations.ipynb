{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.37 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-openai==0.1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(\n",
    "    provider=\"openai\", \n",
    "    model=\"gpt-4o\", \n",
    "    api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "setup_embed_model(\n",
    "    provider=\"openai\", \n",
    "    api_key=OPENAI_API_KEY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eval_dataset = load_dataset(\"harpreetsahota/LI_Learning_RAG_Eval_Set\", split='train')\n",
    "\n",
    "eval_dataset = eval_dataset.filter(lambda x: x['question_groundedness_score'] is not None and x['question_groundedness_score'] >= 4)\n",
    "\n",
    "smol_eval_set = eval_dataset.shuffle(seed=2022).select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_documents_from_docstore\n",
    "\n",
    "senpai_documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Qdrant Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.storage.index_store.simple_index_store import SimpleIndexStore\n",
    "from llama_index.core.settings import Settings\n",
    "from utils import setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"words-of-the-senpai-rr-fusion\"\n",
    "\n",
    "rr_fusion_vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME, enable_hybrid=True)\n",
    "\n",
    "rr_fusion_storage_context = StorageContext.from_defaults(\n",
    "    docstore = SimpleDocumentStore.from_persist_dir(persist_dir=\"../data/words-of-the-senpais\"),\n",
    "    index_store=SimpleIndexStore(),\n",
    "    vector_store = rr_fusion_vector_store\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest with a docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "from utils import ingest \n",
    "\n",
    "sentence_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=16)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=senpai_documents, \n",
    "    embed_model=Settings.embed_model,\n",
    "    storage_context=rr_fusion_storage_context,\n",
    "    transformations=[sentence_splitter]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief word on vector store query modes\n",
    "\n",
    "The vector_store_query_mode in LlamaIndex determines the type of search to be performed. Here's a brief description of each mode:\n",
    "\n",
    " - `default`: This mode performs a vector search. It retrieves the most similar vectors based on the query vector.  They create a numerical representation of a piece of text, represented as a long list of numbers. These dense vectors can capture rich semantics across the entire piece of text. `alpha=0.75` is used by default.\n",
    "\n",
    " - `hybrid`: This mode performs a hybrid search. It combines vector search with traditional search methods. `alpha` parameter determines weighting (`alpha = 0` -> bm25, `alpha = 1` -> vector search). \n",
    "\n",
    " - `semantic_hybrid`: Semantic hybrid search combines text search with vector embeddings. Text search provides keyword matching and lexical retrieval. Vector embeddings allow finding documents with similar meaning, even if they don't contain exact keyword matches. This mode incorporates semantic reranking to hybrid search results to improve search relevance.\n",
    "\n",
    " - `sparse`: Most of the elements in a sparse vector are zero, with only a few key values being non-zero. These sparse vectors are great at capturing specific keywords and similar small details. You need to use a specialized embedding model to create sparse vectors. \n",
    "   - `FastEmbed` has a few choices for sparse text embedding models, for example you can pass in `prithvida/Splade_PP_en_v1` as the model name when you run `setup_embed_model` if you want to use them. \n",
    "    - We didn't use a sparse vector here, so we won't see this in action.  \n",
    "    - Note, if you try this you'll need to set the `sparse_top_k` argument, which represents how many nodes will be retrieved from each dense and sparse query. For example, if `sparse_top_k=5` is set, that means I will retrieve 5 nodes using sparse vectors and 5 nodes using dense vectors.\n",
    "\n",
    " - `text_search`: Text search looks for exact keyword matches between the query and documents.\n",
    "\n",
    " - `similarity_top_k`: controls the final number of returned nodes. A fusion algorithm is applied to rank and order the nodes from different vector spaces, `similarity_top_k=2` means the top two nodes after fusion are returned.\n",
    "\n",
    " - `hybrid_top_k`: return top k results from `hybrid` search. `similarity_top_k` is used for dense search top k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_STRING = \"How can I create my own luck?\"\n",
    "\n",
    "def test_retrievers(query=QUERY_STRING, index=index, **kwargs):\n",
    "    retriever_engine = index.as_retriever(**kwargs)\n",
    "    retrieved_docs = retriever_engine.retrieve(query)\n",
    "    print(f\"Retrieved {len(retrieved_docs)} nodes.\")\n",
    "    print(\"\\n\")\n",
    "    for node in retrieved_docs:\n",
    "        print(f\"Score: {node.score:.2f} - {node.text}...\\n-----\\n\")\n",
    "    \n",
    "mode_kwargs = {\n",
    "    'default': {'vector_store_query_mode': 'default', 'similarity_top_k': 3},\n",
    "    'bm25': {'vector_store_query_mode':'hybrid', 'alpha': 0.0, 'hybrid_top_k': 3}, \n",
    "    'hybrid': {'vector_store_query_mode':'hybrid', 'alpha': 0.25, 'hybrid_top_k': 3},\n",
    "    'semantic_hybrid': {'vector_store_query_mode':'semantic_hybrid', 'alpha': 0.75, 'hybrid_top_k': 3},\n",
    "    # 'sparse': {\"sparse_top_k\":5},\n",
    "    'text_search': {'vector_store_query_mode':'text_search', 'similarity_top_k': 3},\n",
    "}\n",
    "\n",
    "for mode, kwargs in mode_kwargs.items():\n",
    "    print(f\"Retrieving nodes using: {mode} retrieval\")\n",
    "    test_retrievers(**kwargs)\n",
    "    print(f\"Retrieval with {mode} complete...\")        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Transformation\n",
    "\n",
    "When handling user queries in a RAG system, agent, or any other pipeline, there are various ways to transform and decompose the queries before executing them.\n",
    "\n",
    "One way is query rewriting. This involves rewriting the original query in multiple ways while which then sent sent for retrieval and generation. \n",
    "\n",
    "LlamaIndex implements various query transformations, [check the source code for details](https://github.com/run-llama/llama_index/blob/f116d75557d6867ed2cc61811a1c2f0b0c4d4ddb/llama-index-legacy/llama_index/legacy/indices/query/query_transform/base.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "QUERY_GEN_PROMPT = \"\"\"Users aren't always the best at articulating what they're looking for. Your task is to understand the \n",
    "essense of the user query and generate {num_queries} alternate queries to expand the users query so it's more robust. This way the user will\n",
    "recieve the most relevant information. \n",
    "\n",
    "Examples are delimited by triple backticks (```) below\n",
    "\n",
    "````\n",
    "User Query: How can I find the positive in situations that seem negative?\n",
    "\n",
    "Alternate Queries:\n",
    "\n",
    "1. How can I cultivate optimism and positive thinking in my daily life?\n",
    "2. Is it possible to find meaning and purpose in challenging or difficult times?\n",
    "3. What are some effective strategies for reframing negative thoughts into positive ones?\n",
    "````\n",
    "\n",
    "````\n",
    "User Query: How do I deal with setbacks, failures, delays, defeat, or other disasters?\n",
    "\n",
    "Alternate Queries:\n",
    "\n",
    "1. How can I build resilience and learn to cope with adversity effectively?\n",
    "2. What are some practical tips for overcoming challenges and obstacles that I face?\n",
    "3. How can I develop a growth mindset and view setbacks as opportunities for learning?\n",
    "4. What are healthy ways to process and learn from failures and mistakes?\n",
    "````\n",
    "````\n",
    "User Query: How can I overcome defeat and suffering by changing my mindset?\n",
    "\n",
    "Alternate Queries:\n",
    "\n",
    "1. What is the power of positive thinking and affirmations, and how can they benefit me?\n",
    "2. Can mindfulness and meditation practices improve my mental well-being and outlook?\n",
    "3. How can I develop self-compassion and acceptance, especially during difficult times?\n",
    "```\n",
    "\n",
    "Generate {num_queries} alternate queries, one on each line, for the following user query:\\n\n",
    "--------------------\n",
    "User Query: {query}\\n\n",
    "--------------------\n",
    "\n",
    "Alternate Queries:\\n\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "QUERY_GEN_PROMPT_TEMPLATE = PromptTemplate(QUERY_GEN_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(query= QUERY_STRING, llm=Settings.llm, num_queries  = 4):\n",
    "    response = llm.predict(\n",
    "        QUERY_GEN_PROMPT_TEMPLATE, \n",
    "        num_queries=num_queries, \n",
    "        query=query\n",
    "        )\n",
    "    queries = response.split(\"\\n\")\n",
    "    queries_str = \"\\n\".join(queries)\n",
    "    print(f\"Generated queries:\\n{queries_str}\")\n",
    "    return queries\n",
    "\n",
    "generate_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Fusion Retriever\n",
    "\n",
    "The Hybrid Fusion Retriever combines of semantic and keyword-based approaches.  This uses a [BM25-based retriever](https://en.wikipedia.org/wiki/Okapi_BM25) with a semantic index. BM25 is a ranking function used by search engines to estimate the relevance of documents to a given search query. \n",
    "\n",
    "#### How it works\n",
    "\n",
    "The system follows a three-step process:\n",
    "\n",
    "- **Query Generation/Rewriting**: It creates multiple queries from the original user query to better match the user's intent and improve the precision and recall of the retrieved results.\n",
    "\n",
    "- **Retrieval**: It performs the retrieval for each query over an ensemble of retrievers.\n",
    "\n",
    "- **Reranking/Fusion**: It combines the results from all queries and applies a reranking step to fuse the top relevant results.\n",
    "\n",
    "#### ℹ️ Useful knowledge to have as a RAG practitioner\n",
    "\n",
    "##### Index Fusion Mode\n",
    "\n",
    "We set the mode to `reciprocal_rerank`. The system merges its index with a BM25 based retriever. This allows it to understand both the semantic relationships (meaningful connections between words) and keywords in the input queries. Other modes are `relative_score`, `dist_based_score`, `simple` .\n",
    "\n",
    "  - [`reciprocal_rerank`](https://github.com/run-llama/llama_index/blob/f116d75557d6867ed2cc61811a1c2f0b0c4d4ddb/llama-index-core/llama_index/core/retrievers/fusion_retriever.py#L99): Reciprocal rank is a measure of how early a relevant item appears in a ranked list. Lower ranks correspond to higher relevance. This mode fuses the results from multiple sources by giving higher importance to nodes that appear earlier in the rankings across those sources.\n",
    "\n",
    "  - [`relative_score`](https://github.com/run-llama/llama_index/blob/f116d75557d6867ed2cc61811a1c2f0b0c4d4ddb/llama-index-core/llama_index/core/retrievers/fusion_retriever.py#L135): It scales each score to a range from 0 to 1 using min-max scaling. Then it multiplies each scaled score by a retriever-specific weight. After that, it divides each score by the total number of queries. Basically, it scales, weights, and combines scores from multiple retrieval sources.\n",
    "\n",
    "  - `dist_based_score`: Same as `relative_score`, but, instead of using the minimum and maximum scores directly, the function calculates them based on the mean and standard deviation of the scores. This reduces the impact of outliers on the scaling process.\n",
    "\n",
    "  - `simple`: re-orders results based on original scores\n",
    "\n",
    "\n",
    "##### **Reciprocal Rerank Algorithm**\n",
    "\n",
    " Since both retrievers calculate a score for the relevance of results, the system uses the reciprocal rerank algorithm to reshuffle the results. This is done without employing additional models or excessive computation, making the process more efficient.\n",
    " \n",
    "  - 🧮 **Rank Calculation**: For each unique node, calculate its reciprocal rank from each list where it appears. The reciprocal rank of a node in a list is defined as 1 divided by its position in that list (e.g., a node at rank 3 has a reciprocal rank of 1/3).\n",
    "\n",
    "  - 📊 **Score Aggregation**: Sum up the reciprocal ranks for each node across all lists in which it appears. This aggregated score represents the overall relevance of the node, taking into account its performance across multiple retrieval scenarios.\n",
    "\n",
    "  - 🥇🥈🥉 **Reordering**: Finally, reorder all nodes based on their aggregated scores, from highest to lowest. This re-ranking step prioritizes nodes that consistently appear in higher ranks across multiple lists, thus likely to be more relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(docstore=index.docstore, similarity_top_k=5)\n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=5,\n",
    "    num_queries=3,  # set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    query_gen_prompt=QUERY_GEN_PROMPT_TEMPLATE, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_scores = retriever.retrieve(\n",
    "    \"How can I stop wasting energy on projecting a facade and focus on expanding my potential as a human being?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes_with_scores:\n",
    "    print(f\"Score: {node.score:.2f} - {node.text}...\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "\n",
    "from utils import create_query_pipeline\n",
    "from utils import run_generations_on_eval_set\n",
    "\n",
    "from prompts import HYPE_ANSWER_GEN_PROMPT\n",
    "\n",
    "HYPE_ANSWER_GEN_PROMPT_TEMPLATE = PromptTemplate(HYPE_ANSWER_GEN_PROMPT)\n",
    "\n",
    "rr_fusion_query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever,\n",
    "    response_mode = ResponseMode.COMPACT_ACCUMULATE,\n",
    "    use_async = True,\n",
    "    text_qa_template = HYPE_ANSWER_GEN_PROMPT_TEMPLATE\n",
    "    )\n",
    "\n",
    "rr_fusion_chain = [Settings.llm,  rr_fusion_query_engine]\n",
    "\n",
    "rr_fusion_query_pipeline = create_query_pipeline(rr_fusion_chain)\n",
    "\n",
    "smol_eval_set = run_generations_on_eval_set(\n",
    "    eval_dataset=smol_eval_set, \n",
    "    col_name=\"rr-fusion-answer\", \n",
    "    query_pipeline=rr_fusion_query_pipeline,\n",
    "    time_out=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in smol_eval_set.select(range(10)):\n",
    "    print(\"💬\\n\")\n",
    "    print(f\"\"\"🙋🏽‍♂️ Question: {row[\"question\"]}\"\"\")\n",
    "    print(f\"\"\"\"RR Fusion Reponse: {row[\"rr-fusion-answer\"]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `SubQuestionQueryEngine`\n",
    "\n",
    "The `SubQuestionQueryEngine` works by breaking down a complex query into simpler sub-questions (with each potentially targeting a specific data source).\n",
    "\n",
    "#### Here's how it works:\n",
    "\n",
    " - The `SubQuestionQueryEngine` receives a complex query.\n",
    "\n",
    "- It then decomposes this query into several sub-questions. Each sub-question is designed to extract specific information from a particular data source.\n",
    "\n",
    "- The engine then sends these sub-questions to their respective data sources and gathers the responses.\n",
    "\n",
    "- Finally, it synthesizes all the intermediate responses to form a final comprehensive answer to the original complex query.\n",
    "\n",
    "This process makes the `SubQuestionQueryEngine` particularly useful for handling compare/contrast queries across documents, as well as queries pertaining to a specific document. It's also well-suited for multi-document queries and can execute any number of sub-queries against any subset of query engine tools before synthesizing the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=index.as_query_engine(),\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"the senpais\",\n",
    "            description=\"The collective thoughts and writings of all my virtual mentors\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "sub_question_query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    use_async=True\n",
    "    )\n",
    "\n",
    "sub_question_query_engine.update_prompts({'response_synthesizer:text_qa_template':HYPE_ANSWER_GEN_PROMPT_TEMPLATE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_prompt_dict\n",
    "\n",
    "sub_q_prompts = sub_question_query_engine.get_prompts()\n",
    "\n",
    "display_prompt_dict(sub_q_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_question_query_engine.query(\"How can I build my own luck, what are the types of luck I should pursue, and how can I hack luck and minimize my exposure to downside while maintaining skin in the game?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "At a high level, [HyDE](https://arxiv.org/pdf/2212.10496.pdf) is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example. \n",
    "\n",
    "- 🧐 **Problem Tackled**: Addresses the struggle of creating fully zero-shot dense retrieval systems without relevance labels.\n",
    "\n",
    "- 📚 **Traditional Methods**: Rely on relevance labels for document retrieval based on semantic similarities.\n",
    "\n",
    "- 🚫 **Zero-Shot Challenge**: Especially tough without a large dataset for training.\n",
    "\n",
    "### What is HyDE?\n",
    "\n",
    "Given a query, `HyDE` instructs a language model to generate a hypothetical document.\n",
    "\n",
    "This document captures relevance patterns but might contain inaccuracies or false details.\n",
    "\n",
    "After generating the hypothetical document, an unsupervised contrastively learned encoder encodes the document into an embedding vector.\n",
    "\n",
    "This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity.\n",
    "\n",
    "### How Does HyDE Work?\n",
    "\n",
    "The process starts by feeding a query to a generative model with the instruction to \"write a document that answers the question\". This generates a hypothetical document that captures the essence of relevance.\n",
    "\n",
    " - Generates an embedding vector for a \"fake\" document\n",
    "\n",
    "- It does not generate any actual text content for the document\n",
    "\n",
    "- The embedding is solely for reserving space in the vectorstore index\n",
    "\n",
    "- There is no full hypothetical document text you can access later\n",
    "\n",
    "This vector is used to search against the corpus embeddings, and the most similar real documents are retrieved. The idea is that a hypothetical answer to a question is more semantically similar to the real answer than the question is. \n",
    "\n",
    "**In practice this means that your search would use GPT to generate a hypothetical answer, then embed that and use it for search**.\n",
    "\n",
    "Key advantages of HyDE:\n",
    "\n",
    "- Zero-shot, no labeled data or fine-tuning needed\n",
    "\n",
    "- Performs comparably to fine-tuned retrievers across tasks/languages\n",
    "\n",
    "- Grounds the query in real data via generated hypothetical documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "\n",
    "from llama_index.core.query_engine import TransformQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde = HyDEQueryTransform(\n",
    "    include_original=True,\n",
    "    )\n",
    "\n",
    "hyde_query_engine = TransformQueryEngine(\n",
    "    query_engine = index.as_query_engine(), \n",
    "    query_transform = hyde,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prompt_dict(hyde_query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = hyde_query_engine.query(QUERY_STRING)\n",
    "\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_chain = [Settings.llm,  hyde_query_engine]\n",
    "\n",
    "hyde_query_pipeline = create_query_pipeline(hyde_chain)\n",
    "\n",
    "smol_eval_set = run_generations_on_eval_set(\n",
    "    eval_dataset=smol_eval_set, \n",
    "    col_name=\"hyde-answer\", \n",
    "    query_pipeline=hyde_query_pipeline,\n",
    "    time_out=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in smol_eval_set.select(range(10)):\n",
    "    print(\"💬\\n\")\n",
    "    print(f\"\"\"🙋🏽‍♂️ Question: {row[\"question\"]}\"\"\")\n",
    "    print(f\"\"\"\"RR Fusion Reponse: {row[\"rr-fusion-answer\"]}\"\"\")\n",
    "    print(f\"\"\"\"HyDE Reponse: {row[\"hyde-answer\"]}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
