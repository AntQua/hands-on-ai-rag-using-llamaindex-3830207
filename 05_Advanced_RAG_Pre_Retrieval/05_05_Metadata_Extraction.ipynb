{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.25 llama-index-embeddings-fastembed qdrant-client llama-index-vector-stores-qdrant llama-index-llms-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4-turbo-2024-04-09\")\n",
    "\n",
    "setup_embed_model(provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_documents_from_docstore\n",
    "\n",
    "senpai_documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eval_dataset = load_dataset(\"harpreetsahota/LI_Learning_RAG_Eval_Set\", split='train')\n",
    "\n",
    "eval_dataset = eval_dataset.filter(lambda x: x['question_groundedness_score'] is not None and x['question_groundedness_score'] >= 4)\n",
    "\n",
    "smol_eval_set = eval_dataset.shuffle(seed=1217).select(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÉÔ∏è Metadata for Nodes\n",
    "\n",
    "Metadata provides additional context or information about the nodes, which can help in more precise and relevant retrieval of information.\n",
    "\n",
    "During retrieval we can leverage additional context and information, enabling more precise and relevant retrieval of information. However, the effectiveness of this approach depends on the quality and relevance of the metadata tags used.\n",
    "\n",
    "The most simplest way to add metadata is to do so manually. \n",
    "\n",
    "Let's add some metadata for what each of our Senpai's are known for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_for = {\n",
    "    \"Naval Ravikant\": \"Known for his insights on how to build wealth and achieve happiness through developing specific knowledge, embracing accountability, playing long-term games, and understanding the power of compound interest in all areas of life.\",\n",
    "    \"Balaji Srinivasan\": \"Has insights on how to think independently, identify opportunities, and build a better future through the strategic application of technology and clear reasoning.\",\n",
    "    \"Paul Graham\": \"Provides advice on the hacker mindset, arguing that hackers are really makers and creators - akin to painters - who can leverage their unique way of thinking to push boundaries, challenge the status quo, and shape the future through technology and entrepreneurship.\",\n",
    "    \"Nassim Nicholas Taleb\": \"Argues for 'Skin in the Game', that is having a personal stake in the outcome is necessary for fairness as it aligns incentives and exposes individuals to both the potential rewards and risks of their decisions.\",\n",
    "    \"Seneca\": \"Offers timeless advice on how to cultivate wisdom, build mental resilience, and live a life of purpose and contentment by focusing on what is essential, mastering one's emotions, and aligning oneself with nature.\",\n",
    "    \"Bruce Lee\": \"Offers profound wisdom on self-improvement, personal growth, and martial arts philosophy, emphasizing the importance of adaptability, self-expression, and embracing one's own unique path in life, \"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in senpai_documents:\n",
    "    document.metadata['known_for'] = known_for.get(document.metadata['author']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senpai_documents[42].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Automatically Extract Metadata\n",
    "\n",
    "Metadata extraction in LlamaIndex is a process that helps to disambiguate similar-looking passages of text, especially in long documents. \n",
    "\n",
    "This is achieved by using LLMs to extract contextual information relevant to the document. This information aids the retrieval and language models in distinguishing between similar passages.\n",
    "\n",
    "In LlamaIndex, metadata extraction is performed using various feature extractors within the [`MetadataExtractor`](https://github.com/run-llama/llama_index/tree/954398e1957027a364d0d332fee61733ad322f8b/llama-index-core/llama_index/core/extractors) class. \n",
    "\n",
    "These extractors include:\n",
    "\n",
    " - `SummaryExtractor`: This extractor automatically generates a summary over a set of Nodes.\n",
    "\n",
    " - `QuestionsAnsweredExtractor`: This extractor identifies a set of questions that each Node can answer.\n",
    "\n",
    " - `TitleExtractor`: This extractor identifies a title over the context of each Node.\n",
    "\n",
    " - `KeywordExtractor`: Keywords that uniquely identify the node\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.extractors import  SummaryExtractor, QuestionsAnsweredExtractor, TitleExtractor, KeywordExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SummaryExtractor().prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(QuestionsAnsweredExtractor().prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TitleExtractor().node_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KeywordExtractor has it's prompt template buried in an LLM call, and not an attribute.\n",
    "\n",
    "Here's what it is in [the source code](https://github.com/run-llama/llama_index/blob/954398e1957027a364d0d332fee61733ad322f8b/llama-index-core/llama_index/core/extractors/metadata_extractors.py#L198):\n",
    "\n",
    "```python\n",
    "f\"\"\"\\\n",
    "{{context_str}}. Give {self.keywords} unique keywords for this \\\n",
    "document. Format as comma separated. Keywords: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Metadata Extraction\n",
    "\n",
    "Let's perform some automated metadata extraction for better retrieval results. \n",
    "\n",
    "We'll employ two extractors: \n",
    "\n",
    " - `QuestionAnsweredExtractor` to generates question/answer pairs from a piece of text\n",
    " \n",
    " - `SummaryExtractor` to extracts summaries, not only within the current text, but also within adjacent texts. \n",
    " \n",
    "This strategy leads to higher quality answer given retrieved results.\n",
    "\n",
    "To do this, we define metadata extractors:\n",
    " \n",
    " - `qa_extractor`\n",
    " \n",
    " - `summary_extractor`\n",
    "\n",
    " Note the use of `MetadataMode.EMBED` this specifies how metadata is handled when generating embeddings for a document or node. When you call the `get_content()` function on a document and specify `MetadataMode.EMBED`, it returns the content of the document with the metadata that is visible to the embedding model.\n",
    "\n",
    " We'll also use `GPT-3.5-Turbo` to generate the metadata.\n",
    "\n",
    "#### üë®üèΩ‚Äçüíª I encourage you to try out the other metadata extractors and see what your results look like. \n",
    "\n",
    "For example, you can try the `KeywordExtractor` or  `TitleExtractor` like so:\n",
    "\n",
    "```python\n",
    "\n",
    "keyword_extractor = KeywordExtractor(keywords=10, llm=llm)\n",
    "\n",
    "title_extractor = TitleExtractor(nodes=5, llm=llm)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.core.extractors import SummaryExtractor, QuestionsAnsweredExtractor\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "qa_llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=256, chunk_overlap=16)\n",
    "\n",
    "qa_extractor = QuestionsAnsweredExtractor(\n",
    "    questions=2, \n",
    "    llm=qa_llm, \n",
    "    metadata_mode=MetadataMode.EMBED,\n",
    "    embed_model=Settings.embed_model,\n",
    "    )\n",
    "\n",
    "summary_extractor = SummaryExtractor(\n",
    "    summaries=[\"prev\", \"self\", \"next\"], \n",
    "    llm=qa_llm,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üë∑üèΩ‚Äç‚ôÇÔ∏è üóÇÔ∏è Build the Index and Ingest to Qdrant\n",
    "\n",
    "In the last few videos we did the node splitting first and then ingested to Qdrant. That was to make the pattern clear to you and give you a sense of how splitting works.\n",
    "\n",
    "But, we can actually just do this kind of stuff directly using the ingetsion pipeline.\n",
    "\n",
    "Note, I will leave it up to you to experiment using with one, or both of the extractors and fiddling with the hyperparameters.\n",
    "\n",
    "The parsing here took ~30 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from utils import create_index, create_query_engine, ingest, setup_vector_store\n",
    "\n",
    "COLLECTION_NAME = \"words-of-the-senpai-qa-plus-summaries-nodes\"\n",
    "\n",
    "qa_summaries_vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "qa_summaries_storage_context = StorageContext.from_defaults(vector_store=qa_summaries_vector_store)\n",
    "\n",
    "qa_summaries_index = create_index(vector_store=qa_summaries_vector_store, storage_context=qa_summaries_storage_context)\n",
    "\n",
    "transforms = [Settings.embed_model, text_splitter, qa_extractor, summary_extractor]\n",
    "\n",
    "qa_summaries = ingest(\n",
    "    documents=senpai_documents,\n",
    "    transformations=transforms,\n",
    "    vector_store=qa_summaries_vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(qa_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_summaries[100].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qa_summaries[100].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Setup Query Engine and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from utils import create_query_engine\n",
    "from prompts import HYPE_ANSWER_GEN_PROMPT\n",
    "\n",
    "HYPE_ANSWER_GEN_PROMPT_TEMPLATE = PromptTemplate(HYPE_ANSWER_GEN_PROMPT)\n",
    "\n",
    "qa_summaries_query_engine = create_query_engine(\n",
    "    index=qa_summaries_index, \n",
    "    mode=\"query\",\n",
    "    response_mode=\"compact\",\n",
    "    similiarty_top_k=5,\n",
    "    vector_store_query_mode=\"mmr\", \n",
    "    vector_store_kwargs={\"mmr_threshold\": 0.42},\n",
    "    )\n",
    "\n",
    "qa_summaries_query_engine.update_prompts({'response_synthesizer:text_qa_template':HYPE_ANSWER_GEN_PROMPT_TEMPLATE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_pipeline\n",
    "\n",
    "qa_summaries_chain = [Settings.llm,  qa_summaries_query_engine]\n",
    "\n",
    "qa_summaries_query_pipeline = create_query_pipeline(qa_summaries_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_generations_on_eval_set\n",
    "\n",
    "smol_eval_set = run_generations_on_eval_set(\n",
    "    eval_dataset=eval_dataset, \n",
    "    col_name=\"qa-summaries-answer\", \n",
    "    query_pipeline=qa_summaries_query_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in smol_eval_set:\n",
    "    print(\"üí¨\\n\")\n",
    "    print(f\"\"\"üôãüèΩ‚Äç‚ôÇÔ∏è Question: {row[\"question\"]}\"\"\")\n",
    "    print(f\"\"\"\"üß†ü§ñ Reponse: {row[\"qa-summaries-answer\"]}\"\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
