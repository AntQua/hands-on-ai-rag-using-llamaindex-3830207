{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.22 llama-index-embeddings-openai qdrant-client llama-index-vector-stores-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from getpass import getpass\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CO_API_KEY'] = getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "Settings.llm = Cohere(model=\"command-r\")\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion Pipeline\n",
    "\n",
    "- üîÑ **IngestionPipeline Overview**: Utilizes `Transformations` applied to input data, modifying data into nodes, which are returned or inserted to a vector database.\n",
    "\n",
    "- üíæ **Caching Mechanism**: Each node+transformation pair is cached, enhancing efficiency for identical subsequent operations by utilizing cached results.\n",
    "\n",
    "\n",
    "### Using an `IngestionPipeline`\n",
    "\n",
    "First, let's read in some data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files = [\"../data/it_can_be_done.txt\"], \n",
    "    filename_as_id=True).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion Pipeline with Document Management\n",
    "\n",
    "\n",
    " ‚Ä¢  üíæ **Caching in IngestionPipeline**: Hashes and stores each node + transformation combination to speed up future processes with identical data.\n",
    "\n",
    " ‚Ä¢  üìÅ **Local Cache Management**: The input nodes list and transformation pair are cached in the pipeline. When we apply the same transformation to that list of nodes again, the output nodes are retrieved from the cache.\n",
    "\n",
    " ‚Ä¢  üìö **Docstore Attachment**:  Enables document management in the ingestion pipeline, using `doc_id` or `node.ref_doc_id` for identification. Prevents running a transformation on the same document multiple times by using the document ID and the hash of the document content to manage duplicates.\n",
    "\n",
    " ‚Ä¢  üóÇÔ∏è **Duplicate Handling**:\n",
    "  - Maintains a `doc_id` to `document_hash` map to identify duplicates.\n",
    "\n",
    "  - Re-processes documents if the same `doc_id` is found with a changed hash.\n",
    "\n",
    "  - Skips documents if the same `doc_id` is found but the hash remains unchanged.\n",
    "\n",
    " ‚Ä¢  üö´ **Without Vector Store**:\n",
    "  - Limited to checking and removing duplicate inputs.\n",
    "\n",
    " ‚Ä¢  ‚ú® **With Vector Store**:\n",
    "  - Enables handling of upserts for updated documents, offering advanced management capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL, \n",
    "    api_key=QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    collection_name=\"it_can_be_done\")\n",
    "\n",
    "ingest_cache = IngestionCache(\n",
    "    collection=\"it_can_be_done\",\n",
    ")\n",
    "\n",
    "# create pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        TokenTextSplitter(chunk_size=256, chunk_overlap=16),\n",
    "        Settings.embed_model\n",
    "    ],\n",
    "    docstore=SimpleDocumentStore(),\n",
    "    vector_store=vector_store,\n",
    "    cache=ingest_cache,\n",
    ")\n",
    "\n",
    "# run the pipeline\n",
    "nodes = pipeline.run(documents = documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingestion pipeline allows for saves the cache and docstore to a default folder `(./pipeline_storage)`. \n",
    "\n",
    "When running the pipeline, it reuses the cache, skips duplicate documents in the docstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.persist('./pipeline_storage')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
