{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# the first line of imports you should already have installed if you are running this notebook locally \n",
    "# !pip install llama-index==0.10.20 llama-index-embeddings-openai qdrant-client llama-index-vector-stores-qdrant\n",
    "!pip install llama-index-storage-kvstore-redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import nest_asyncio\n",
    "\n",
    "from tqdm import tqdm \n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion Pipeline\n",
    "\n",
    "- üîÑ **IngestionPipeline Overview**: Utilizes `Transformations` applied to input data, modifying data into nodes, which are returned or inserted to a vector database.\n",
    "\n",
    "- üíæ **Caching Mechanism**: Each node+transformation pair is cached, enhancing efficiency for identical subsequent operations by utilizing cached results.\n",
    "\n",
    "\n",
    "### Using an `IngestionPipeline`\n",
    "\n",
    "First, let's read in some data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files = [\"../data/it_can_be_done.txt\"], filename_as_id=True).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion Pipeline with Document Management\n",
    "\n",
    " ‚Ä¢  üíæ **Caching in IngestionPipeline**: Hashes and stores each node + transformation combination to speed up future processes with identical data.\n",
    "\n",
    " ‚Ä¢  üìÅ **Local Cache Management**: The input nodes list and transformation pair are cached in the pipeline. When we apply the same transformation to that list of nodes again, the output nodes are retrieved from the cache.\n",
    "\n",
    " ‚Ä¢  üìö **Docstore Attachment**:  Enables document management in the ingestion pipeline, using `doc_id` or `node.ref_doc_id` for identification. Prevents running a transformation on the same document multiple times by using the document ID and the hash of the document content to manage duplicates.\n",
    "\n",
    " ‚Ä¢  üóÇÔ∏è **Duplicate Handling**:\n",
    "  - Maintains a `doc_id` to `document_hash` map to identify duplicates.\n",
    "\n",
    "  - Re-processes documents if the same `doc_id` is found with a changed hash.\n",
    "\n",
    "  - Skips documents if the same `doc_id` is found but the hash remains unchanged.\n",
    "\n",
    " ‚Ä¢  üö´ **Without Vector Store**:\n",
    "  - Limited to checking and removing duplicate inputs.\n",
    "\n",
    " ‚Ä¢  ‚ú® **With Vector Store**:\n",
    "  - Enables handling of upserts for updated documents, offering advanced management capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "from llama_index.storage.kvstore.redis import RedisKVStore as RedisCache\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL, \n",
    "    api_key=QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"it_can_be_done\")\n",
    "\n",
    "ingest_cache = IngestionCache(\n",
    "    cache=RedisCache.from_host_and_port(host=\"127.0.0.1\", port=6379),\n",
    "    collection=\"it_can_be_done\",\n",
    ")\n",
    "\n",
    "# create pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        TokenTextSplitter(chunk_size=256, chunk_overlap=16),\n",
    "        OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "    ],\n",
    "    docstore=SimpleDocumentStore(),\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "# run the pipeline\n",
    "nodes = pipeline.run(documents = documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"What does Burton Braley says about starting?\"\n",
    ")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
