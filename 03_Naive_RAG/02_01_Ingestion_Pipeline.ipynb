{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.20 llama-index-embeddings-openai qdrant-client llama-index-vector-stores-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "from tqdm import tqdm \n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion Pipeline\n",
    "\n",
    "- üîÑ **IngestionPipeline Overview**: Utilizes `Transformations` applied to input data, modifying data into nodes, which are returned or inserted to a vector database.\n",
    "\n",
    "- üíæ **Caching Mechanism**: Each node+transformation pair is cached, enhancing efficiency for identical subsequent operations by utilizing cached results.\n",
    "\n",
    "\n",
    "### How to use an `IngestionPipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for Project Gutenberg texts\n",
    "base_url = \"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt\"\n",
    "\n",
    "# Directory to save the downloaded files\n",
    "directory = Path(\"gutenberg_books\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate a list of book IDs to download\n",
    "book_ids = range(1, 20)\n",
    "\n",
    "# Generate URLs for each book ID\n",
    "urls = [base_url.format(book_id=book_id) for book_id in book_ids]\n",
    "\n",
    "# Download each file and save it in the specified directory\n",
    "for url in tqdm(urls, desc=\"Downloading books\"):  # Wrap urls with tqdm for a progress bar\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the filename from the URL using the book ID and create a file name\n",
    "        book_id = url.split('/')[-2]  # Extracts the book ID from the URL\n",
    "        filename = f\"pg{book_id}.txt\"\n",
    "        file_path = directory / filename\n",
    "        # Save the file to the specified directory\n",
    "        file_path.write_text(response.text)\n",
    "    else:\n",
    "        tqdm.write(f\"Failed to download {url}. HTTP status code: {response.status_code}\")  # Use tqdm.write for messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"gutenberg_books\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "\n",
    "# create pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=128, chunk_overlap=16),\n",
    "        TitleExtractor(),\n",
    "        OpenAIEmbedding(model=\"text-embedding-3-small\",dimensions=512)\n",
    "    ],\n",
    ")\n",
    "\n",
    "# run the pipeline\n",
    "nodes = pipeline.run(documents = documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "client = qdrant_client.QdrantClient(path=\"gutenberg_books/qdrant\")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"gutenberg_books\")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=128, chunk_overlap=16),\n",
    "        TitleExtractor(),\n",
    "        OpenAIEmbedding(model=\"text-embedding-3-small\",dimensions=512)\n",
    "    ],\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "pipeline.run(documents = documents)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching\n",
    "\n",
    "- üíæ **Caching in IngestionPipeline**: Hashes and stores each node + transformation combination to expedite future processes with identical data.\n",
    "\n",
    "- üìÅ **Local Cache Management**: Guides on storing and loading pipeline cache for enhanced efficiency and convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "pipeline.persist(\"gutenberg_books/pipeline_storage\")\n",
    "\n",
    "# load and restore state\n",
    "new_pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=256, chunk_overlap=32),\n",
    "        TitleExtractor(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "new_pipeline.load(\"gutenberg_books/pipeline_storage\")\n",
    "\n",
    "# will run instantly due to the cache\n",
    "nodes = new_pipeline.run(documents=documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
