{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Chunk Size\n",
    "\n",
    "> **The Chunking Commandment:** Your goal is not to chunk for chunking sake, our goal is to get our data in a format where it can be retrieved for value later.\n",
    ">\n",
    "> -- Greg Kamradt, [5 Levels Of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n",
    "\n",
    "\n",
    "When building a RAG system, you must understand how documents are processed and indexed. \n",
    "\n",
    "Chunking is the most fundamental aspect of this process. In this lesson, we'll explore what chunking is, how it affects the indexing and retrieval process, and how you can customize chunk size and overlap to optimize your results.\n",
    "\n",
    "### Understanding Chunking\n",
    "\n",
    "When documents are ingested into an index, `LlamaIndex` splits them into smaller pieces called \"chunks.\"  This process is known as chunking. By default, LlamaIndex uses a chunk size of 1024 and a chunk overlap of 20. \n",
    "\n",
    "But what do these numbers mean, and how do they impact the indexing and retrieval process?\n",
    "\n",
    "#### Chunk Size\n",
    "\n",
    "The chunk size determines the maximum number of tokens (roughly equivalent to words) that each chunk will contain. With a default chunk size of 1024, `LlamaIndex` will split your documents into chunks that are no longer than 1024 tokens each.\n",
    "\n",
    "*   **ü§è Smaller Chunk Size**\n",
    "    *   More precise and focused embeddings\n",
    "    *   Beneficial for retrieving specific information\n",
    "\n",
    "*   **üëêLarger Chunk Size**\n",
    "    *   More general embeddings with broader context\n",
    "    *   Useful for document overviews, but may miss details\n",
    "\n",
    "#### Chunk Overlap\n",
    "\n",
    "*   **üîó Chunk Overlap** \n",
    "    *   Shared tokens between adjacent chunks (default: 20)\n",
    "    *   Maintains context and prevents information loss\n",
    "\n",
    "I recommend taking a look at [this chunk visualizer](https://huggingface.co/spaces/m-ric/chunk_visualizer) to get an intuitive sense for chunk size and overlap.\n",
    "\n",
    "#### ü§î The Impact of Chunk Size\n",
    " - **üìè Relevance and Granularity**\n",
    "    *   Smaller chunks (e.g., 128) offer granularity but risk missing vital information, or lack sufficient context.\n",
    "    *   Larger chunks (e.g., 512) are more likely to capture necessary context, but also run the risk of including irrelevant information.\n",
    "    *   Faithfulness and Relevancy metrics help assess response quality. \n",
    "\n",
    " -  **üéØ Chunk Size and Use Case**\n",
    "    *   **Question Answering:** Shorter, specific chunks for precise answers.\n",
    "    *   **Summarization:** Longer chunks to capture the overall context.\n",
    "\n",
    " - **‚è≥ Response Generation Time**\n",
    "    *   Larger chunks provide more context but may slow down the system.\n",
    "    *   Balancing comprehensiveness with speed is crucial.\n",
    "    \n",
    " - **ü§∑üèΩ‚Äç‚ôÇÔ∏è Finding the Optimal Size ‚öñÔ∏è**\n",
    "    *   Testing various chunk sizes is essential for specific use cases and datasets. \n",
    "    *   Balancing information capture with efficiency is key.\n",
    "\n",
    "\n",
    "\n",
    "### Considerations When Customizing Chunk Size\n",
    "\n",
    "When deciding on a chunk size, there are a few things to keep in mind:\n",
    "\n",
    "| Factor | Description |\n",
    "|--------|-------------|\n",
    "| üìÑ **Data Characteristics** | The optimal chunk size may depend on the type of data you're indexing. For example, if you're indexing long, detailed documents, you may want to use a larger chunk size to capture more context. If you're indexing short, focused passages, a smaller chunk size may be more appropriate. |\n",
    "| üîç **Retrieval Requirements** | Consider what kind of information you need to retrieve from your index. If you need to retrieve very specific details, a smaller chunk size may be better. If you're looking for more general information, a larger chunk size may suffice. |\n",
    "| üî¢ **Similarity Parameters** | When using a larger chunk size with a `VectorStoreIndex`, you may also want to increase the `similarity_top_k` parameter. This parameter determines how many of the most similar chunks are retrieved for each query. With larger chunks, you may need to retrieve more chunks to cover the same amount of information. |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
