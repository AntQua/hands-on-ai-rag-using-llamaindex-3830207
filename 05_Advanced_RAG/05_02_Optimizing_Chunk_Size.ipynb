{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.25 llama-index-embeddings-fastembed qdrant-client llama-index-vector-stores-qdrant llama-index-llms-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4e33a75a474cb8a3da6ded7250b3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from utils import setup_llm, setup_embed_model\n",
    "\n",
    "setup_llm(api_key=CO_API_KEY)\n",
    "\n",
    "setup_embed_model(provider=\"fastembed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_documents_from_docstore\n",
    "\n",
    "senpai_documents = get_documents_from_docstore(\"../data/words-of-the-senpais\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb87f2dbb1f4ea6ac9bc751c70ead9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/316 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eval_dataset = load_dataset(\"harpreetsahota/LI_Learning_RAG_Eval_Set\", split='train')\n",
    "\n",
    "eval_dataset = eval_dataset.filter(lambda x: x['question_groundedness_score'] is not None and x['question_groundedness_score'] >= 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Chunk Size\n",
    "\n",
    "In this lesson, we'll explore what chunking is, how it affects the indexing and retrieval process, and how you can customize chunk size and overlap to optimize your results.\n",
    "\n",
    "> **The Chunking Commandment:** Your goal is not to chunk for chunking sake, our goal is to get our data in a format where it can be retrieved for value later.\n",
    ">\n",
    "> -- Greg Kamradt, [5 Levels Of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n",
    "\n",
    "## Understanding Chunking\n",
    "\n",
    "When documents are ingested into an index, `LlamaIndex` splits them into smaller pieces called \"chunks.\"  This process is known as chunking. By default, LlamaIndex uses a *chunk size* of 1024 and a *chunk overlap* of 20. \n",
    "\n",
    "But what do these numbers mean, and how do they impact the indexing and retrieval process?\n",
    "\n",
    "### Chunk Size\n",
    "\n",
    "The chunk size determines the maximum number of tokens (roughly equivalent to words) that each chunk will contain. With a default chunk size of 1024, `LlamaIndex` will split your documents into chunks that are no longer than 1024 tokens each.\n",
    "\n",
    "#### **🤏 Smaller Chunk Size**\n",
    "\n",
    "*   More precise and focused embeddings\n",
    "\n",
    "*   Beneficial for retrieving specific information\n",
    "\n",
    "#### **👐Larger Chunk Size**\n",
    "\n",
    "*   More general embeddings with broader context\n",
    "\n",
    "*   Useful for document overviews, but may miss details\n",
    "\n",
    "### Chunk Overlap\n",
    "\n",
    "*   Shared tokens between adjacent chunks (default: 20)\n",
    "\n",
    "*   Maintains context and prevents information loss\n",
    "\n",
    "I recommend taking a look at [this chunk visualizer](https://huggingface.co/spaces/m-ric/chunk_visualizer) to get an intuitive sense for chunk size and overlap.\n",
    "\n",
    "## 🤔 The Impact of Chunk Size\n",
    " \n",
    " #### **📏 Relevance and Granularity**\n",
    "\n",
    "*   Smaller chunks (e.g., 128) offer granularity but risk missing vital information, or lack sufficient context.\n",
    "\n",
    "*   Larger chunks (e.g., 512) are more likely to capture necessary context, but also run the risk of including irrelevant information.\n",
    "\n",
    "*   Faithfulness and Relevancy metrics help assess response quality. \n",
    "\n",
    " #### **🎯 Chunk Size and Use Case**\n",
    "\n",
    "*   **Question Answering:** Shorter, specific chunks for precise answers.\n",
    "\n",
    "*   **Summarization:** Longer chunks to capture the overall context.\n",
    "\n",
    " #### **⏳ Response Generation Time**\n",
    "\n",
    "*   Larger chunks provide more context but may slow down the system.\n",
    "\n",
    "*   Balancing comprehensiveness with speed is crucial.\n",
    "    \n",
    " #### **⚖️ Finding the Optimal Size**\n",
    "\n",
    "*   Testing various chunk sizes is essential for specific use cases and datasets. \n",
    "\n",
    "*   Balancing information capture with efficiency is key.\n",
    "\n",
    "### Considerations When Customizing Chunk Size\n",
    "\n",
    "When deciding on a chunk size, there are a few things to keep in mind:\n",
    "\n",
    "| Factor | Description |\n",
    "|--------|-------------|\n",
    "| 📄 **Data Characteristics** | The optimal chunk size depends on the data you're indexing. Long, detailed documents, may require a larger chunk size to capture more context. Smaller chunk size may be more appropriate for short, focused passages. |\n",
    "| 🔍 **Retrieval Requirements** | If you need to retrieve very specific details, a smaller chunk size may be better. If you're looking for more general information, a larger chunk size may suffice. |\n",
    "| 🔢 **Similarity Parameters** | When using a larger chunk size with a `VectorStoreIndex`, you may also want to increase the `similarity_top_k` parameter. This parameter determines how many of the most similar chunks are retrieved for each query. With larger chunks, you may need to retrieve more chunks to cover the same amount of information. |\n",
    "\n",
    "### There are [various methods](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules) you can use to chunk your documents. \n",
    "\n",
    "| Parser Type | Splitter Name | Description |\n",
    "|-------------|---------------|-------------|\n",
    "| 📁 File-Based Node Parsers | 📄`SimpleFileNodeParser` | The simplest flow: `FlatFileReader` + `SimpleFileNodeParser` which automatically use the best node parser for each type of content. Then, you may want to chain the file-based node parser with a text-based node parser to account for the actual length of the text. |\n",
    "| | 🌐`HTMLNodeParser` | This node parser uses beautifulsoup to parse raw HTML. By default, it will parse a select subset of HTML tags, but you can override this. The default tags are: [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"b\", \"i\", \"u\", \"section\"] |\n",
    "| | 🎭`JSONNodeParser` | The `JSONNodeParser` parses raw JSON. |\n",
    "| | 📝`MarkdownNodeParser` | The `MarkdownNodeParser` parses raw markdown text. |\n",
    "| ✂️ Text-Splitters | 💻`CodeSplitter` | Splits raw code-text based on the language it is written in. |\n",
    "| | 🦜🔗`LangchainNodeParser` | You can also wrap any existing text splitter from langchain with a node parser. |\n",
    "| | 📜`SentenceSplitter` | The `SentenceSplitter` attempts to split text while respecting the boundaries of sentences. |\n",
    "| | 🪟`SentenceWindowNodeParser` | The `SentenceWindowNodeParser` splits all documents into individual sentences. The resulting nodes also contain the surrounding \"window\" of sentences around each node in the metadata.|\n",
    "| | 🧠`SemanticSplitterNodeParser` | Instead of chunking text with a fixed chunk size, the semantic splitter adaptively picks the breakpoint in-between sentences using embedding similarity. This ensures that a \"chunk\" contains sentences that are semantically related to each other. |\n",
    "| | 🪙`TokenTextSplitter` | The `TokenTextSplitter` attempts to split to a consistent chunk size according to raw token counts. |\n",
    "| 🔗 Relation-Based Node Parsers | 🌿`HierarchicalNodeParser` | This node parser will chunk nodes into hierarchical nodes. This means a single input will be chunked into several hierarchies of chunk sizes, with each node containing a reference to it's parent node. |\n",
    "\n",
    "\n",
    "## We're only going to focus on a few strategies\n",
    "\n",
    "I'll show you how to split/chunk test using each method belwo. \n",
    "\n",
    "Then, I'll randomly select one configuration one the strategies below and we'll evaluate against our evaluation set. \n",
    "\n",
    "Just one evaluations in total, you can do the rest on your own.\n",
    "\n",
    " - 🪙`TokenTextSplitter`\n",
    " \n",
    " - 📜`SentenceSplitter`\n",
    "\n",
    " - 🪟`SentenceWindowNodeParser`\n",
    "\n",
    " - 🧠`SemanticSplitterNodeParser`\n",
    "\n",
    "# 🪙 `TokenTextSplitter`\n",
    "\n",
    "The primary function is to divide a given text into smaller chunks, ensuring each chunk stays within a specified token limit. \n",
    "\n",
    "### **How it Works**\n",
    "\n",
    "1.  **Tokenization:** It utilizes a tokenizer to break down the text into individual tokens (words or subwords).  The default tokenizer is the `tiktoken` tokenizer for GPT-3.5-Turbo.\n",
    "\n",
    "2.  **Chunking:** It then groups these tokens into chunks, ensuring each chunk's size is within the defined `chunk_size` limit. \n",
    "\n",
    "3.  **Overlap Handling:** To maintain context and coherence between chunks, it can incorporate an overlap, specified by `chunk_overlap`, where the last few tokens of one chunk are repeated at the beginning of the next.\n",
    "\n",
    "### Arguments you need to know\n",
    "\n",
    "*   **`chunk_size`**: Controls the maximum token count for each chunk. Defualts to 1024.\n",
    "\n",
    "*   **`chunk_overlap`**: Determines the number of overlapping tokens between consecutive chunks. Defaults to 20.\n",
    "\n",
    "*   **`separator`**: Specifies the primary character used to split the text into words. Defaults to space (`\" \"`).\n",
    "\n",
    "*   **`backup_separators`**: Provides additional characters for splitting if the primary separator isn't sufficient. Defaults to new line character (`\"\\n\"`).\n",
    "\n",
    "*   **`include_metadata`**: Enables or disables the inclusion of metadata within each chunk. Defaults to `True`.\n",
    "\n",
    "* **`include_prev_next_rel`**: Enables or disables tracking the relationship between nodes. Defaults to `True`.\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "The basic usage pattern is as follows (you don't need to pass anything if you want to keep the default values.):\n",
    "\n",
    "```python\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter()\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "```\n",
    "\n",
    "\n",
    "Let's evaluate the impact varying chunk size has on our `ragas` metrics. I'll limit our exploration to the `chunk_sizes = [128, 256, 512, 1024]` and hold `chunk_overlap` fixed to 16 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set a very high hourly aspirational rate for yourself and stick to it. It should seem and feel absurdly high. If it doesnt, its not high enough. Whatever you picked, my advice to you would be to raise it. Like I said, for myself, even before I had money, for the longest time I used $5,000 an hour. And if you extrapolate that out into what it looks like as an annual salary, its multiple millions of dollars per year. Ironically, I actually think Ive beaten it. Im not the hardest working personIm actually a lazy person. I work through bursts of energy where Im really motivated with something. If I actually look at how much Ive earned per actual hour that Ive put in, its probably quite a bit higher than that. Can you expand on your statement, If you secretly despise wealth, it will elude you? If you get into a relative mindset, youre always going to hate people who do better than you, youre always going to be jealous or envious of them. Theyll sense those feelings when you try and do business with them. When you try and do business with somebody, if you have any bad thoughts or any judgments about them, they will feel it. Humans are wired to feel what the other person deep down inside feels. You have to get out of a relative mindset. Literally, being anti-wealth will prevent you from becoming wealthy, because you will not have the right mindset for it, you wont have the right spirit, and you wont be dealing with people on the right level. Be optimistic, be positive. Its important. Optimists actually do better in the long run.\n"
     ]
    }
   ],
   "source": [
    "print(documents[42].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Set a very high hourly aspirational rate for yourself and stick to it. It should seem and feel absurdly high. If it doesnt, its not high enough. Whatever you picked, my advice to you would be to raise it. Like I said, for myself, even before I had money, for the longest time',\n",
       " 'I said, for myself, even before I had money, for the longest time I used $5,000 an hour. And if you extrapolate that out into what it looks like as an annual salary, its multiple millions of dollars per year. Ironically, I actually think Ive beaten it. Im not the hardest working',\n",
       " 'year. Ironically, I actually think Ive beaten it. Im not the hardest working personIm actually a lazy person. I work through bursts of energy where Im really motivated with something. If I actually look at how much Ive earned per actual hour that Ive put in, its probably quite a bit higher than that. Can you',\n",
       " 'that Ive put in, its probably quite a bit higher than that. Can you expand on your statement, If you secretly despise wealth, it will elude you? If you get into a relative mindset, youre always going to hate people who do better than you, youre always going to be jealous or envious of',\n",
       " 'who do better than you, youre always going to be jealous or envious of them. Theyll sense those feelings when you try and do business with them. When you try and do business with somebody, if you have any bad thoughts or any judgments about them, they will feel it. Humans are wired to feel what',\n",
       " 'any judgments about them, they will feel it. Humans are wired to feel what the other person deep down inside feels. You have to get out of a relative mindset. Literally, being anti-wealth will prevent you from becoming wealthy, because you will not have the right mindset for it, you wont have the right',\n",
       " 'because you will not have the right mindset for it, you wont have the right spirit, and you wont be dealing with people on the right level. Be optimistic, be positive. Its important. Optimists actually do better in the long run.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "TokenTextSplitter(chunk_size=64, chunk_overlap=16).split_text(documents[42].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "def token_splitter(chunk_size, documents):\n",
    "    splitter = TokenTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=16,\n",
    "        )\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_splitter_results = {}\n",
    "\n",
    "# Iterate over each chunk size and perform token splitting\n",
    "for size in chunk_sizes:\n",
    "    key = f\"token_split_chunk_size_{size}\"\n",
    "    token_splitter_results[key] = token_splitter(size, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With chunk_size_128 we get 8669 chunks.\n",
      "With chunk_size_256 we get 3812 chunks.\n",
      "With chunk_size_512 we get 1938 chunks.\n",
      "With chunk_size_1024 we get 1884 chunks.\n"
     ]
    }
   ],
   "source": [
    "for key, value in token_splitter_results.items():\n",
    "    print(f\"With {key} we get {len(value)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📜`SentenceSplitter`\n",
    "\n",
    "The `SentenceSplitter` class, as its name suggests, specializes in splitting text while trying to keep complete sentences and paragraphs together. This is in contrast to the `TokenTextSplitter`, which focuses on token limits.\n",
    "\n",
    "### How it Works\n",
    "\n",
    "1. **Initial Splitting:**\n",
    "\n",
    "    *   The text is first divided into paragraphs using the specified `paragraph_separator` (defaults to triple newline characters `\"\\n\\n\\n\"`).\n",
    "\n",
    "    *   Each paragraph is then further split using a \"chunking tokenizer\" (defaults to [`PunktSentenceTokenizer`](https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html) from the `nltk` library). Which basically looks for sentences boundaries.\n",
    "\n",
    "    *   If these methods don't yield enough splits, it resorts to a backup regex and the default separators (`CHUNKING_REGEX = \"[^,.;。？！]+[,.;。？！]?\"`).\n",
    "\n",
    "2. **Chunking with Sentence Awareness:**\n",
    "\n",
    "    *   The resulting splits are grouped into chunks, keeping sentences together as much as possible. \n",
    "\n",
    "    *   It considers the `is_sentence` flag for each split during this process.\n",
    "\n",
    "    *   Chunk size and overlap still play a role, but sentence boundaries are given preference.\n",
    "\n",
    "3. **Overlap Handling:**\n",
    "\n",
    "    *   Similar to `TokenTextSplitter`, it incorporates overlap between chunks to maintain context. \n",
    "\n",
    "    *   However, it prioritizes using the last complete sentence for overlap rather than just the last few tokens.\n",
    "\n",
    "### Arguments you need to know\n",
    "\n",
    "*   **`chunk_size`**: The target token size for each chunk.\n",
    "\n",
    "*   **`chunk_overlap`**: The number of overlapping tokens between chunks.\n",
    "\n",
    "*   **`separator`**: The default separator for splitting (e.g., space).\n",
    "\n",
    "*   **`paragraph_separator`**: The string used to identify paragraph breaks.\n",
    "\n",
    "*   **`secondary_chunking_regex`**: A backup regex for splitting if the primary methods are insufficient.\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "```python\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=256, chunk_overlap=50)\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "```\n",
    "\n",
    "### When to Use SentenceSplitter\n",
    "\n",
    "*   When preserving complete sentences and paragraphs is essential for understanding the context.\n",
    "\n",
    "*   When dealing with text where sentence boundaries are meaningful (e.g., legal documents, narratives).\n",
    "\n",
    "*   When you want to avoid having broken sentences at the beginning or end of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Set a very high hourly aspirational rate for yourself and stick to it. It should seem and feel absurdly high. If it doesnt, its not high enough. Whatever you picked, my advice to you would be to raise it.',\n",
       " 'Whatever you picked, my advice to you would be to raise it. Like I said, for myself, even before I had money, for the longest time I used $5,000 an hour.',\n",
       " 'And if you extrapolate that out into what it looks like as an annual salary, its multiple millions of dollars per year. Ironically, I actually think Ive beaten it. Im not the hardest working personIm actually a lazy person. I work through bursts of energy where Im really motivated with something.',\n",
       " 'I work through bursts of energy where Im really motivated with something. If I actually look at how much Ive earned per actual hour that Ive put in, its probably quite a bit higher than that. Can you expand on your statement, If you secretly despise wealth, it will elude you?',\n",
       " 'If you get into a relative mindset, youre always going to hate people who do better than you, youre always going to be jealous or envious of them. Theyll sense those feelings when you try and do business with them.',\n",
       " 'Theyll sense those feelings when you try and do business with them. When you try and do business with somebody, if you have any bad thoughts or any judgments about them, they will feel it. Humans are wired to feel what the other person deep down inside feels.',\n",
       " 'Humans are wired to feel what the other person deep down inside feels. You have to get out of a relative mindset.',\n",
       " 'You have to get out of a relative mindset. Literally, being anti-wealth will prevent you from becoming wealthy, because you will not have the right mindset for it, you wont have the right spirit, and you wont be dealing with people on the right level. Be optimistic, be positive.',\n",
       " 'Be optimistic, be positive. Its important. Optimists actually do better in the long run.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "SentenceSplitter(chunk_size=64, chunk_overlap=16).split_text(documents[42].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_splitter(chunk_size, documents):\n",
    "    splitter = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=16,\n",
    "        )\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_splitter_results = {}\n",
    "\n",
    "# Iterate over each chunk size and perform sentence splitting\n",
    "for size in chunk_sizes:\n",
    "    key = f\"sentence_split_chunk_size_{size}\"\n",
    "    sentence_splitter_results[key] = sentence_splitter(size, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sentence_split_chunk_size_128 we get 9150 chunks.\n",
      "With sentence_split_chunk_size_256 we get 4018 chunks.\n",
      "With sentence_split_chunk_size_512 we get 1937 chunks.\n",
      "With sentence_split_chunk_size_1024 we get 1884 chunks.\n"
     ]
    }
   ],
   "source": [
    "for key, value in sentence_splitter_results.items():\n",
    "    print(f\"With {key} we get {len(value)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: `TokenTextSplitter` vs `SentenceSplitter`\n",
    "\n",
    "`TokenTextSplitter` splits the text into chunks based on a specified number of tokens. It uses a tokenizer to break down the text into individual tokens (words or subwords), and then groups these tokens into chunks of a specified size. If the text doesn't divide evenly into the specified chunk size, the last chunk will contain the remaining tokens, which could be less than the specified chunk size.\n",
    "\n",
    "`SentenceSplitter`, on the other hand, splits the text into chunks based on sentences. It uses a sentence boundary detection algorithm to identify where sentences begin and end, and then groups these sentences into chunks. The size of these chunks can vary depending on the length of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Randomly select a key from the chunk_size_results dictionary\n",
    "random_key = random.choice(list(chunk_size_results.keys()))\n",
    "print(f\"Randomly selected key: {random_key}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
