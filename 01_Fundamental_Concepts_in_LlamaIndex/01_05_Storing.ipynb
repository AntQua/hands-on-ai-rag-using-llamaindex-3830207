{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index llama-index-embeddings-openai qdrant-client llama-index-vector-stores-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing\n",
    "\n",
    "Loading and indexing data costs time and money.\n",
    "\n",
    "By default, indexed data is stored in memory. But, you can store your data to avoid the time and costs associated with re-indexing them.  The simplest way to do this **persisting to disk**.\n",
    "\n",
    "Each `Index` object has a `.persist()` method, which will write all the data to disk at the specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def create_directory(directory_name):\n",
    "    path = Path(directory_name)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Directory '{directory_name}' created successfully.\")\n",
    "\n",
    "create_directory(\"data\")\n",
    "\n",
    "create_directory(\"persisted_storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P data https://www.gutenberg.org/cache/epub/10763/pg10763.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've dowloaded data, let's:\n",
    "\n",
    "1) Load as Document\n",
    "2) Parse as Nodes\n",
    "3) Create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load as document\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "file_path = \"data/pg10763.txt\"\n",
    "\n",
    "document = SimpleDirectoryReader(input_files=[file_path], filename_as_id=True).load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse as nodes\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=512, \n",
    "    chunk_overlap=16,\n",
    "    paragraph_separator=\"\\n\\n\\n\\n\" \n",
    ")\n",
    "\n",
    "nodes = sentence_splitter.get_nodes_from_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import  VectorStoreIndex\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "index = VectorStoreIndex(nodes, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.persist()` method of the index to store the indexed data to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"persisted_storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can avoid re-loading and re-indexing your data by loading the persisted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"persisted_storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If the index is initialized with a custom `transformation`, `embed_model`, etc. then you need to pass those same options during `load_index_from_storage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = load_index_from_storage(storage_context, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Vector Database\n",
    "\n",
    "We'll use qdrant as our vector database of choice throughout this course.\n",
    "\n",
    "To use qdrant to store embeddings from the `VectorStoreIndex`, you need to:\n",
    "\n",
    "- Initialize the qdrant client\n",
    "\n",
    "- Create a `Collection` to store your data in qdrant\n",
    "\n",
    "- Assign qdrant as the `vector_store` in a `StorageContext`\n",
    "\n",
    "- Initialize your `VectorStoreIndex` using that `StorageContext`\n",
    "\n",
    "Below, we initialize a `QdrantClient` for interacting with qdrant, an open-source vector store. \n",
    "\n",
    "We use `location=\":memory:\"` for in-memory operations, ideal for quick, lightweight experiments without needing an external qdrant deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "create_directory(\"persisted_storage/qdrant\")\n",
    "\n",
    "# initialize qdrant client\n",
    "client = qdrant_client.QdrantClient(\n",
    "    path=\"persisted_storage/qdrant\"\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    collection_name=\"it_can_be_done\",\n",
    "    prefer_grpc=True\n",
    ")\n",
    "\n",
    "# assign qdrant vector store to storage context\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store)\n",
    "\n",
    "# create the index\n",
    "index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sneak peek at querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.query(\"What lessons can be learned from the poems about success?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.query(\"Does this book include a poem titled Start Where You Stand?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserting Documents or Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "my_poem = \"\"\"Beneath the endless sky, where stars do meet the eye,\n",
    "A path of inner peace, where truth and duty lie.\n",
    "In the heart of a Sikh, where the Guru's wisdom flows,\n",
    "And in the Stoic's mind, where calm reflection grows.\n",
    "\n",
    "\"Accept what life may bring,\" the Stoic gently teaches,\n",
    "\"Embrace your fate with grace,\" their quiet wisdom reaches.\n",
    "The Sikh, in vibrant faith, sees God in all, not some,\n",
    "In service and in love, their kindred spirits come.\n",
    "\n",
    "Through storms and tranquil days, their journey is the same,\n",
    "To live with virtue's light, and keep alive the flame.\n",
    "For in the end, it's not the riches or the fame,\n",
    "But how we played the game, and honored life's true name.\n",
    "\n",
    "So, let us walk this path, with courage, love, and grace,\n",
    "United in our quest, in this vast human race.\n",
    "A Sikh Stoic's heart, where peace and duty blend,\n",
    "On this eternal road, that winds and never ends.\n",
    "\"\"\"\n",
    "\n",
    "poem_document = Document(text=my_poem)\n",
    "\n",
    "index.insert(poem_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Are there any poems about Sikh and Stoic philosophy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's persist the `qdrant` index to disk for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"persisted_storage/qdrant\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
