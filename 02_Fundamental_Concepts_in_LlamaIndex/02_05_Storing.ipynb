{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.25 llama-index-embeddings-cohere llama-index-llms-cohere qdrant-client llama-index-vector-stores-qdrant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÑÔ∏è Storing\n",
    "\n",
    "Loading and indexing data costs time and money.\n",
    "\n",
    "By default, indexed data is stored in memory. But, you can store your data to avoid the time and costs associated with re-indexing them.  The simplest way to do this **persisting to disk**.\n",
    "\n",
    "Each `Index` object has a `.persist()` method, which will write all the data to disk at the specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def create_directory(directory_name):\n",
    "    path = Path(directory_name)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Directory '{directory_name}' created successfully.\")\n",
    "\n",
    "create_directory(\"data\")\n",
    "\n",
    "create_directory(\"persisted_storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P data https://www.gutenberg.org/cache/epub/10763/pg10763.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've dowloaded data, let's:\n",
    "\n",
    "1) Load as Document\n",
    "2) Parse as Nodes\n",
    "3) Create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load as document\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "file_path = \"data/pg10763.txt\"\n",
    "\n",
    "document = SimpleDirectoryReader(input_files=[file_path], filename_as_id=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse as nodes\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=512, \n",
    "    chunk_overlap=16,\n",
    "    paragraph_separator=\"\\n\\n\\n\\n\" \n",
    ")\n",
    "\n",
    "nodes = sentence_splitter.get_nodes_from_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.core import  VectorStoreIndex\n",
    "\n",
    "embed_model = CohereEmbedding(model_name=\"embed-english-v3.0\")\n",
    "\n",
    "index = VectorStoreIndex(nodes, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.persist()` method of the index to store the indexed data to disk. Now you can avoid re-loading and re-indexing your data by loading the persisted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"persisted_storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÉÔ∏è Storege Context\n",
    "\n",
    "`StorageContext` in `LlamaIndex` is a core abstraction that revolves around the storage of `Nodes`, indices, and vectors. \n",
    "\n",
    "It is a utility container that includes the following:\n",
    "\n",
    " - `docstore`: A [`BaseDocumentStore`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/storage/docstore/types.py) for storing nodes.\n",
    "\n",
    " - `index_store`: A [`BaseIndexStore`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/storage/index_store/types.py#L13) for storing indices.\n",
    "\n",
    " - `vector_store`: A [`VectorStore`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/vector_stores/simple.py) for storing vectors.\n",
    "\n",
    " - `graph_store`: A [`GraphStore`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/graph_stores/simple.py) for storing knowledge graphs.\n",
    "\n",
    "The `StorageContext` can be created with default settings, comprising a document store, index store, vector store, and graph store. It enables storing the context in a specific directory, facilitating data storage and retrieval from disk to save time and indexing expenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"persisted_storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚òÅÔ∏è Using a Vector Database\n",
    "\n",
    "We'll use qdrant as our vector database of choice throughout this course.\n",
    "\n",
    "To use qdrant to store embeddings from the `VectorStoreIndex`, you need to:\n",
    "\n",
    "- Initialize the qdrant client\n",
    "\n",
    "- Create a `Collection` to store your data in qdrant\n",
    "\n",
    "- Assign qdrant as the `vector_store` in a `StorageContext`\n",
    "\n",
    "- Initialize your `VectorStoreIndex` using that `StorageContext`\n",
    "\n",
    "Below, we initialize a `QdrantClient` for interacting with qdrant, an open-source vector store. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "create_directory(\"persisted_storage/qdrant\")\n",
    "\n",
    "# initialize qdrant client\n",
    "client = qdrant_client.QdrantClient(\n",
    "    path=\"persisted_storage/qdrant\"\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, \n",
    "    collection_name=\"it_can_be_done\",\n",
    "    prefer_grpc=True\n",
    ")\n",
    "\n",
    "# assign qdrant vector store to storage context\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store)\n",
    "\n",
    "# create the index\n",
    "index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    embed_model=embed_model,\n",
    "    storage_context=storage_context\n",
    ")\n",
    "\n",
    "# You can persist the `qdrant` index to disk for future use\n",
    "index.storage_context.persist(persist_dir=\"persisted_storage/qdrant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü™É Retrieval\n",
    "\n",
    "A `Retriever` is an interface exposed by the `Index`. An `Index` with its `Retriever` is used for storing and fetching data. The `Retriever` is a part of the `Index` and is used to retrieve the data stored in the Index.\n",
    "\n",
    "\n",
    "### LlamaIndex provides [many different types of retrievers](https://github.com/run-llama/llama_index/tree/main/llama-index-core/llama_index/core/retrievers) to fetch relevant information from ingested data based on a given query. \n",
    "\n",
    "Some examples include\n",
    "\n",
    "### Vector Retriever\n",
    "\n",
    "The vector retriever uses vector similarity search to find the most relevant nodes (chunks of text) based on the query embedding. It requires a vector database like to store and search through the node embeddings.\n",
    "\n",
    "### [Fusion Retriever](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/retrievers/fusion_retriever.py)\n",
    "\n",
    "The fusion retriever generates multiple queries from the original query, performs retrieval over an ensemble of retrievers for each query, and then fuses and reranks the results across all queries. This aims to better capture the query intent through query rewriting and ensembling.\n",
    "\n",
    "### [Recursive Retriever](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/retrievers/recursive_retriever.py)\n",
    "\n",
    "The recursive retriever allows for hierarchical retrieval by first retrieving coarse nodes and then recursively retrieving finer-grained nodes within those coarse nodes. This can be useful for multi-level indexing and retrieval.\n",
    "\n",
    "You can also combine retrievers in interesting ways and build out more advanced retrieval strategies, as we will see later in this course.\n",
    "\n",
    "\n",
    "### In the example here, we're using a Vector Retriever\n",
    "\n",
    " - üîç When searching, your query is also converted into a vector embedding. \n",
    " \n",
    "- üóÇÔ∏è The `VectorStoreIndex` then performs a mathematical operation to rank embeddings based on semantic similarity to your query.\n",
    "\n",
    "- üîù Top-k semantic retrieval is the simplest wasy to query a vector index.\n",
    "\n",
    "- ‚©¨ You can also apply a similarity threshold  (e.g., only return results that are more similar than some value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retirever = index.as_retriever(\n",
    "    similarity_top_k=5,\n",
    "    similarity_threshold=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retirever.retrieve(\"What lessons can be learned from the poems about success?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, chances are you don't just want the returned documents. You want the documents to be synthesized into a response. \n",
    "\n",
    "So, let's build on this pattern in the next lesson and see how we can get a response based on those retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the client so you're not locked out of the index\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
