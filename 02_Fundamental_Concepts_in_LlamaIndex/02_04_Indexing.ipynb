{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è Indexing\n",
    "\n",
    "In `LlamaIndex`, an `Index` is a data structure that contains `Document` objects, allowing for efficient querying by an LLM. There are several types of indexes, but we'll focus on the most commun, `VectorStoreIndex`.\n",
    "\n",
    "- üåê **Vector Store Index:** Segments your `Documents` into `Nodes` and generates vector embeddings for each node's text, prepping them for retrieval.\n",
    "\n",
    "- üîÑ **Vector Store Index Process:** Converts all your text into embeddings with an API from your LLM, aka \"embedding your text.\"\n",
    "\n",
    "### ‚öôÔ∏è Embedding Text\n",
    "\n",
    "First, let's see what an embedding is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "te_small = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "te_large = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "\n",
    "te_ada = OpenAIEmbedding(model=\"text-embedding-ada-002\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use local embedding models, by using an embedding model from Hugging Face.\n",
    "\n",
    "```python\n",
    "\n",
    "pip install llama-index-embeddings-huggingface\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "hf_embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"A\"\n",
    "\n",
    "string_2 = \"This is a complete sentence.\"\n",
    "\n",
    "string_3 = \"\"\"In the pursuit of a life well-lived, one must recognize the transient nature of the \n",
    "material world and the enduring value of virtue. The Sikh Gurus taught us that the Divine Light \n",
    "resides within all, and thus, we are united in our essence beyond the superficial distinctions of \n",
    "caste, creed, or status. Similarly, the Stoics emphasized the cultivation of inner virtues such as courage, \n",
    "temperance, and wisdom, understanding that true freedom lies in mastery over one's own perceptions and actions. \n",
    "As we navigate the vicissitudes of life, let us remember that our choices are our own, and in choosing virtue, \n",
    "we align ourselves with the cosmic order and the teachings of the Gurus. It is through selfless service, \n",
    "compassion, and the relentless pursuit of truth that we may attain a state of inner peace and contribute \n",
    "to the harmony of the world, embodying the principles of both Sikhism and Stoicism in our daily lives\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_embedding = te_small.get_text_embedding(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_dimensions(embed_model, list_of_strings):\n",
    "    embeddings = embed_model.get_text_embedding_batch(list_of_strings)   \n",
    "    embed_lens = []\n",
    "    for embedding in embeddings:\n",
    "        embed_lens.append(len(embedding))\n",
    "    return embed_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embedding_dimensions(te_small, [string, string_2, string_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embedding_dimensions(te_large, [string, string_2, string_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embedding_dimensions(te_ada, [string, string_2, string_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_ada.similarity(\n",
    "    te_ada.get_text_embedding(\"\"\"In embracing both the wisdom of the Sikh Gurus and the Stoic philosophers, \n",
    "                              we find a path to tranquility by accepting what is beyond our control and focusing \n",
    "                              our efforts on living virtuously and with purpose.\"\"\"), \n",
    "    te_ada.get_text_embedding(string_2),\n",
    "    mode=\"cosine\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Index\n",
    "\n",
    "First, let's get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def load_text_from_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches and returns the text content from the specified URL.\n",
    "\n",
    "    Parameters:\n",
    "    - url: The URL of the text file to fetch.\n",
    "\n",
    "    Returns:\n",
    "    - The text content of the file if the request is successful; otherwise, an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # This will raise an HTTPError if the response was an error\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Failed to load content from {url}. Error: {e}\"\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/10763/10763.txt\"\n",
    "\n",
    "text_content = load_text_from_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚è≥ Generating embeddings can be time-consuming, especially with large volumes of text, due to numerous API calls required. \n",
    "\n",
    "Now, create an index by passing a **list of Documents**. To save time, and cost, we will only use 10,000 characters of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex\n",
    "\n",
    "full_document = Document(text=text_content)\n",
    "\n",
    "partial_document = Document(text=text_content[50000:60000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 83.77it/s]\n",
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  5.98it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    # remember, you must pass a list of documents!\n",
    "    [partial_document], \n",
    "    embed_model=te_small,\n",
    "    show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you can also build an index over a **list of `Node` objects**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 15.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# instantiate a node parser\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=16,\n",
    "    paragraph_separator=\"\\n\\n\\n\\n\",\n",
    ")\n",
    "\n",
    "# pass a list of documents to the node paraser\n",
    "nodes = splitter.get_nodes_from_documents([partial_document])\n",
    "\n",
    "# create the index from the nodes\n",
    "index_from_nodes = VectorStoreIndex(\n",
    "    nodes,\n",
    "    embed_model=te_small,\n",
    "    show_progress=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval from `VectorStoreIndex`\n",
    "\n",
    " - üîç When searching, your query is also converted into a vector embedding. \n",
    " \n",
    "- The `VectorStoreIndex` then performs a mathematical operation to rank embeddings based on semantic similarity to your query.\n",
    "\n",
    "- üîù Top-k semantic retrieval is the simplest wasy to query a vector index. \n",
    "\n",
    "We'll discuss querying in it's own section!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
