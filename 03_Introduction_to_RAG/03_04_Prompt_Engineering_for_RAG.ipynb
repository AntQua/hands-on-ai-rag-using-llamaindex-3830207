{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index==0.10.25 llama-index-embeddings-fastembed qdrant-client llama-index-vector-stores-qdrant llama-index-llms-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, create_vector_store_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from utils import setup_llm, setup_embed_model, create_vector_store_index\n",
    "\n",
    "setup_llm(api_key=CO_API_KEY)\n",
    "\n",
    "setup_embed_model()\n",
    "\n",
    "COLLECTION_NAME = \"it_can_be_done\"\n",
    "\n",
    "index = create_vector_store_index(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_prompt_dict, create_query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = create_query_engine(\n",
    "    index,\n",
    "    similarity_top_k=3, \n",
    "    return_sources=True\n",
    "    )\n",
    "\n",
    "display_prompt_dict(query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "custom_prompt = \"\"\"You are an assistant for question-answering tasks related to \\\n",
    "motivational poetry. Your must reponse with an original Haiku style poem.\n",
    "\n",
    "Use the following pieces of retrieved context to answer the user's query:\n",
    "\n",
    "---------------------\\n\n",
    "{context_str}\\n\n",
    "---------------------\\n\n",
    "\n",
    "Query: {query_str}\n",
    "\"\"\"\n",
    "\n",
    "custom_prompt_template = PromptTemplate(custom_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": custom_prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prompt_dict(query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_pipeline\n",
    "\n",
    "chain = [Settings.llm, query_engine]\n",
    "\n",
    "query_pipeline = create_query_pipeline(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_pipeline.run(\"I want to turn a new chapter in my life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Synthesizers\n",
    "\n",
    "The Llama Index [documentation](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/) has a lot of detail regarding each of the response sythensizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.response_synthesizers import ResponseMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(ResponseMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚öóÔ∏è Refine \n",
    "\n",
    "Refine is an iterative method to generate a response. \n",
    "\n",
    "Initially, we use the context in the first node and the query to create a basic answer. Then, we refine this answer by inputting it, along with the query and context of the second node, into a \"refine prompt\" to generate an improved answer. \n",
    "\n",
    "This refinement process continues through N-1 nodes, with N being the total number of nodes.\n",
    "\n",
    "#### ü§è Compact\n",
    "\n",
    "Compact and refine mode first combine text chunks into larger consolidated chunks that more fully utilize the available context window, then refine answers across them. This mode is faster than refine since we make fewer calls to the LLM.\n",
    "\n",
    "\n",
    "#### üìù Simple summarize\n",
    "\n",
    "Merge all text chunks into one and make a large language model call. The call will fail if the merged text chunk exceeds the context window size.\n",
    "\n",
    "#### üå¥ Tree summarize\n",
    "\n",
    "Construct a tree index for the candidate nodes in a bottom-up manner then use a summary prompt based on the query. Return the root node as the final response.\n",
    "\n",
    "#### ü§ñ Generation\n",
    "\n",
    "Ignore context, just use LLM to generate a response.\n",
    "\n",
    "#### ‚ùå No text\n",
    "Return the retrieved context nodes, without synthesizing a final response.\n",
    "\n",
    "#### üìè Accumulate\n",
    "\n",
    "Synthesize a response for each text chunk, and then return the concatenation.\n",
    "\n",
    "#### Compact accumulate\n",
    "\n",
    "In the compact and accumulate mode, text chunks are combined into larger chunks to utilize the context window better. Answers are then accumulated for each chunk and returned as a concatenation. This mode is faster than accumulate as it reduces calls to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "\n",
    "query_engine = create_query_engine(\n",
    "    index,\n",
    "    response_synthesizer = response_synthesizer\n",
    "    )\n",
    "\n",
    "chain = [Settings.llm, query_engine]\n",
    "\n",
    "query_pipeline = create_query_pipeline(chain)\n",
    "\n",
    "query_pipeline.run(\"I want to turn a new chapter in my life.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
