{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets llama-index==0.10.37 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-cohere==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you should install the following packages to your environment:\n",
    "\n",
    "`pip install datasets`\n",
    "\n",
    "`pip install llama-index-embeddings-fastembed`\n",
    "\n",
    "`pip install llama-index-llms-mistralai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /opt/conda/envs/lil_llama_index/lib/python3.10/site-\n",
      "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, create_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store, create_index\n",
    "\n",
    "setup_llm(provider=\"cohere\", model=\"command-r-plus\", api_key=CO_API_KEY)\n",
    "\n",
    "setup_embed_model(provider=\"openai\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "COLLECTION_NAME = \"it_can_be_done\"\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = create_index(from_where=\"vector_store\", vector_store=vector_store, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_prompt_dict, create_query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " **Prompt Key**: response_synthesizer:text_qa_template\n",
       "**Text:**\n",
       "```\n",
       "Context information is below.\n",
       "---------------------\n",
       "{context_str}\n",
       "---------------------\n",
       "Given the context information and not prior knowledge, answer the query.\n",
       "Query: {query_str}\n",
       "Answer: \n",
       "```\n",
       "\n",
       "**Prompt Key**: response_synthesizer:refine_template\n",
       "**Text:**\n",
       "```\n",
       "The original query is as follows: {query_str}\n",
       "We have provided an existing answer: {existing_answer}\n",
       "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
       "------------\n",
       "{context_msg}\n",
       "------------\n",
       "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
       "Refined Answer: \n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = create_query_engine(\n",
    "    index=index,\n",
    "    similarity_top_k=3, \n",
    "    mode=\"query\",\n",
    "    return_sources=True\n",
    "    )\n",
    "\n",
    "display_prompt_dict(query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "custom_prompt = \"\"\"You are an assistant for question-answering tasks related to \\\n",
    "motivational poetry. Your must reponse with an original Haiku style poem.\n",
    "\n",
    "Use the following pieces of retrieved context to answer the user's query:\n",
    "\n",
    "---------------------\\n\n",
    "{context_str}\\n\n",
    "---------------------\\n\n",
    "\n",
    "Query: {query_str}\n",
    "\"\"\"\n",
    "\n",
    "custom_prompt_template = PromptTemplate(custom_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": custom_prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " **Prompt Key**: response_synthesizer:text_qa_template\n",
       "**Text:**\n",
       "```\n",
       "You are an assistant for question-answering tasks related to motivational poetry. Your must reponse with an original Haiku style poem.\n",
       "\n",
       "Use the following pieces of retrieved context to answer the user's query:\n",
       "\n",
       "---------------------\n",
       "\n",
       "{context_str}\n",
       "\n",
       "---------------------\n",
       "\n",
       "\n",
       "Query: {query_str}\n",
       "\n",
       "```\n",
       "\n",
       "**Prompt Key**: response_synthesizer:refine_template\n",
       "**Text:**\n",
       "```\n",
       "The original query is as follows: {query_str}\n",
       "We have provided an existing answer: {existing_answer}\n",
       "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
       "------------\n",
       "{context_msg}\n",
       "------------\n",
       "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
       "Refined Answer: \n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prompt_dict(query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_query_pipeline\n",
    "\n",
    "from llama_index.core.query_pipeline import InputComponent\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "chain = [input_component, query_engine]\n",
    "#chain = [input_component, query_engine, Settings.llm]\n",
    "\n",
    "\n",
    "query_pipeline = create_query_pipeline(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 2b56d89f-57d2-4939-80e5-08d0e2682094 with input: \n",
      "input: If you keep your head when all around you are losing their cool and blaming it on you\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 8561aec4-9e54-482a-8633-d9fe17991a1f with input: \n",
      "input: If you keep your head when all around you are losing their cool and blaming it on you\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = query_pipeline.run(input=\"If you keep your head when all around you are losing their cool and blaming it on you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep your cool, stay calm,\n",
      "Blame and panic rise and fall,\n",
      "Your head, a clear sky.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Synthesizers\n",
    "\n",
    "The Llama Index [documentation](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/) has a lot of detail regarding each of the response sythensizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import ResponseMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACCUMULATE',\n",
       " 'COMPACT',\n",
       " 'COMPACT_ACCUMULATE',\n",
       " 'CONTEXT_ONLY',\n",
       " 'GENERATION',\n",
       " 'NO_TEXT',\n",
       " 'REFINE',\n",
       " 'SIMPLE_SUMMARIZE',\n",
       " 'TREE_SUMMARIZE',\n",
       " '__class__',\n",
       " '__doc__',\n",
       " '__members__',\n",
       " '__module__']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ResponseMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response modes\n",
    "\n",
    "In LlamaIndex, [response modes](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/response_modes/) are used to determine how the system should process and return the results of a query.  Each response mode is designed to handle different types of queries and use cases, providing flexibility and customization in how you interact with your data.\n",
    "\n",
    "\n",
    "\n",
    "#### ⚗️ Refine \n",
    "\n",
    "Refine is an iterative method to generate a response. \n",
    "\n",
    "Initially, we use the context in the first node and the query to create a basic answer. Then, we refine this answer by inputting it, along with the query and context of the second node, into a \"refine prompt\" to generate an improved answer. \n",
    "\n",
    "This refinement process continues through N-1 nodes, with N being the total number of nodes. It makes a separate LLM call per Node/retrieved chunk. This mode is good for generating more detailed answers.\n",
    "\n",
    "#### 🤏 Compact\n",
    "\n",
    "Compact and refine mode first combine text chunks into larger consolidated chunks that more fully utilize the available context window, then refine answers across them. This mode is faster than refine since we make fewer calls to the LLM. \n",
    "\n",
    "This mode is useful when you want to reduce the number of LLM calls while still refining the answer.\n",
    "\n",
    "\n",
    "#### 📝 Simple summarize\n",
    "\n",
    "Merge all text chunks into one and make a large language model call. The call will fail if the merged text chunk exceeds the context window size.\n",
    "\n",
    "It's good for quick summarization purposes, but may lose detail due to truncation.\n",
    "\n",
    "#### 🌴 Tree summarize\n",
    "\n",
    "Construct a tree index for the candidate nodes in a bottom-up manner then use a summary prompt based on the query. Return the root node as the final response. When this mode is set, the system is instructed to iterate through many, if not all, documents in order to synthesize an answer, which can lead to better summarization results. \n",
    "\n",
    "This mode is particularly useful for summarization queries, where the goal is to provide a comprehensive summary of a collection of text or a specific topic.\n",
    "\n",
    "#### 🤖 Generation\n",
    "\n",
    "Ignore context, just use LLM to generate a response.\n",
    "\n",
    "#### ❌ No text\n",
    "\n",
    "This mode only runs the retriever to fetch the nodes that would have been sent to the LLM, without actually synthesizing a final response. The nodes can then be inspected by checking `response.source_nodes`.\n",
    "\n",
    "#### 📏 Accumulate\n",
    "\n",
    "This mode applies the query to each text chunk while accumulating the responses into an array. It returns a concatenated string of all responses. \n",
    "\n",
    "This mode is good for when you need to run the same query separately against each text chunk.\n",
    "\n",
    "#### Compact accumulate\n",
    "\n",
    "In the compact and accumulate mode, text chunks are combined into larger chunks to utilize the context window better. Answers are then accumulated for each chunk and returned as a concatenation. This mode is faster than accumulate as it reduces calls to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 27ad43e4-07cd-4879-8d3e-ff095fc1e400 with input: \n",
      "input: What do the poems teach about one should think about success and failure?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module dc24e960-d037-466b-9bfe-db2ca93e408b with input: \n",
      "input: What do the poems teach about one should think about success and failure?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module a6c2444f-ab93-4808-a1ae-8c53605f3c89 with input: \n",
      "messages: The poems teach us that success and failure are not absolute concepts. They are subjective and dependent on one's perspective. What one person may perceive as a failure, another may view as a valuable...\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Absolutely! Failure is often seen as a negative and something to be avoided, but it is an inherent part of life and can offer valuable lessons and insights. It is all a matter of perspective. \\n\\nThe idea of embracing failure is not to glorify it, but to recognize its potential benefits. Failure can foster resilience, innovation, and a more profound understanding of oneself and one\\'s goals. It can also bring about a sense of humility and empathy, connecting us with others who have faced similar struggles. \\n\\nAs the old saying goes, \"What doesn\\'t kill you makes you stronger.\" Failure can be a powerful catalyst for personal growth and development if we are willing to learn from it and adapt. It is all about seeing the silver lining and using setbacks as stepping stones to build a stronger, more resilient, and wiser version of ourselves. \\n\\nHere is a short poem encapsulating this idea: \\n\\n\"The Road to Success\"\\n\\nThe path to success is rarely straight,\\nFull of twists and turns, it\\'s a challenging state.\\nBut failure, they say, is not the end,\\nIt\\'s a chance to learn, to bend, not break or rend.\\n\\nWhat one may perceive as a crushing defeat,\\nIs a lesson in disguise, a chance to retreat,\\nRetreat and regroup, with newfound insight,\\nTo forge a new path with all your might.\\n\\nEmbrace the stumbles, the falls, the mistakes,\\nFor they are the keys to the door that success makes.\\nEach failure a step, each setback a stride,\\nTowards a stronger, wiser you, with resilience as your guide.\\n\\nSo, fear not the failures that life may bring,\\nBut embrace them as friends, and let your spirit sing.\\nFor success is sweetest when earned with toil,\\nAnd failure, a mentor, who guides us to our goal. \\n\\nRemember, failure is not absolute; it is a subjective perception that holds the potential for growth and success.', additional_kwargs={}), raw={'text': 'Absolutely! Failure is often seen as a negative and something to be avoided, but it is an inherent part of life and can offer valuable lessons and insights. It is all a matter of perspective. \\n\\nThe idea of embracing failure is not to glorify it, but to recognize its potential benefits. Failure can foster resilience, innovation, and a more profound understanding of oneself and one\\'s goals. It can also bring about a sense of humility and empathy, connecting us with others who have faced similar struggles. \\n\\nAs the old saying goes, \"What doesn\\'t kill you makes you stronger.\" Failure can be a powerful catalyst for personal growth and development if we are willing to learn from it and adapt. It is all about seeing the silver lining and using setbacks as stepping stones to build a stronger, more resilient, and wiser version of ourselves. \\n\\nHere is a short poem encapsulating this idea: \\n\\n\"The Road to Success\"\\n\\nThe path to success is rarely straight,\\nFull of twists and turns, it\\'s a challenging state.\\nBut failure, they say, is not the end,\\nIt\\'s a chance to learn, to bend, not break or rend.\\n\\nWhat one may perceive as a crushing defeat,\\nIs a lesson in disguise, a chance to retreat,\\nRetreat and regroup, with newfound insight,\\nTo forge a new path with all your might.\\n\\nEmbrace the stumbles, the falls, the mistakes,\\nFor they are the keys to the door that success makes.\\nEach failure a step, each setback a stride,\\nTowards a stronger, wiser you, with resilience as your guide.\\n\\nSo, fear not the failures that life may bring,\\nBut embrace them as friends, and let your spirit sing.\\nFor success is sweetest when earned with toil,\\nAnd failure, a mentor, who guides us to our goal. \\n\\nRemember, failure is not absolute; it is a subjective perception that holds the potential for growth and success.', 'generation_id': '88c30c89-5a75-43d6-b757-8fa28a2e88bb', 'response_id': '09d6c075-e866-491c-9884-78030b14f5fd', 'citations': None, 'documents': None, 'is_search_required': None, 'search_queries': None, 'search_results': None, 'finish_reason': 'COMPLETE', 'tool_calls': None, 'chat_history': [UserMessage(role='USER', message=\"The poems teach us that success and failure are not absolute concepts. They are subjective and dependent on one's perspective. What one person may perceive as a failure, another may view as a valuable lesson or an opportunity for growth. Embracing failure as a stepping stone towards success is key.\", tool_calls=None), ChatbotMessage(role='CHATBOT', message='Absolutely! Failure is often seen as a negative and something to be avoided, but it is an inherent part of life and can offer valuable lessons and insights. It is all a matter of perspective. \\n\\nThe idea of embracing failure is not to glorify it, but to recognize its potential benefits. Failure can foster resilience, innovation, and a more profound understanding of oneself and one\\'s goals. It can also bring about a sense of humility and empathy, connecting us with others who have faced similar struggles. \\n\\nAs the old saying goes, \"What doesn\\'t kill you makes you stronger.\" Failure can be a powerful catalyst for personal growth and development if we are willing to learn from it and adapt. It is all about seeing the silver lining and using setbacks as stepping stones to build a stronger, more resilient, and wiser version of ourselves. \\n\\nHere is a short poem encapsulating this idea: \\n\\n\"The Road to Success\"\\n\\nThe path to success is rarely straight,\\nFull of twists and turns, it\\'s a challenging state.\\nBut failure, they say, is not the end,\\nIt\\'s a chance to learn, to bend, not break or rend.\\n\\nWhat one may perceive as a crushing defeat,\\nIs a lesson in disguise, a chance to retreat,\\nRetreat and regroup, with newfound insight,\\nTo forge a new path with all your might.\\n\\nEmbrace the stumbles, the falls, the mistakes,\\nFor they are the keys to the door that success makes.\\nEach failure a step, each setback a stride,\\nTowards a stronger, wiser you, with resilience as your guide.\\n\\nSo, fear not the failures that life may bring,\\nBut embrace them as friends, and let your spirit sing.\\nFor success is sweetest when earned with toil,\\nAnd failure, a mentor, who guides us to our goal. \\n\\nRemember, failure is not absolute; it is a subjective perception that holds the potential for growth and success.', tool_calls=None)], 'prompt': None, 'meta': ApiMeta(api_version=ApiMetaApiVersion(version='1', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(images=None, input_tokens=57.0, output_tokens=411.0, search_units=None, classifications=None), tokens=ApiMetaTokens(input_tokens=250.0, output_tokens=411.0), warnings=None)}, delta=None, logprobs=None, additional_kwargs={})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "\n",
    "query_engine = create_query_engine(\n",
    "    index,\n",
    "    mode=\"query\",\n",
    "    response_synthesizer = response_synthesizer\n",
    "    )\n",
    "\n",
    "input_component = InputComponent()\n",
    "\n",
    "chain = [input_component, query_engine, Settings.llm,]\n",
    "\n",
    "query_pipeline = create_query_pipeline(chain)\n",
    "\n",
    "query_pipeline.run(input=\"What do the poems teach about one should think about success and failure?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
